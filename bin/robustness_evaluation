#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import datetime
import logging
import os
import shutil
import subprocess
import tempfile

import toml
from tqdm import tqdm

from plant3dvision.compare import *
from plant3dvision.compare import _get_task_fileset
from plantdb import FSDB
from plantdb.fsdb import LOCK_FILE_NAME
from plantdb.fsdb import MARKER_FILE_NAME
from romitask.log import configure_logger

dirname, filename = os.path.split(os.path.abspath(__file__))

CONF_FILE = os.path.join(dirname, "robustness_evaluation.json")
URL = "https://docs.romi-project.eu/plant_imager/developer/pipeline_repeatability/"

DESC = """Robustness evaluation of the Reconstruction & Quantification pipelines.

Evaluating the repeatability of a Reconstruction & Quantification (R&Q) pipeline is made as follows:
 1. duplicate the selected scan dataset in a temporary folder (and clean it from previous R&Q if necessary)
 2. run the R&Q pipeline up to the previous task of the selected task to evaluate, if any
 3. copy/replicate this result to a new database (append a replicate id to the dataset name)
 4. run the task to evaluate for each replicated dataset
 5. compare the directories of the task to evaluate pair by pair
 6. apply the comparison metrics for the task to evaluate, as defined in `robustness_evaluation.json` 

Please note that:
 - Directory comparisons are done at the scale of the files generated by the selected task.
 - We use metrics to get a quantitative comparison on the output of the task.
 - It is possible to create fully independent repetitions by running the whole R&Q pipeline using `-f`.
 - In order to use the ML-based R&Q pipeline, you will have to:
   1. create an output directory
   2. use the `--models` argument to copy the CNN trained models
"""


def parsing():
    # TODO: an option to set a reference (or would that be the job of the Evaluation tasks?!)
    # TODO: use the `pipeline.toml` to defines the previous tasks ?
    # - Load the JSON config file of the script:
    config = json.load(open(CONF_FILE))
    valid_tasks = sorted(set(config.keys()) - set(['Clean']))

    parser = argparse.ArgumentParser(description=DESC,
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     epilog=f"Detailed explanations here: {URL}")

    parser.add_argument("scan", type=str,
                        help="Scan dataset to use for repeatability evaluation.")
    parser.add_argument("task", type=str, default="AnglesAndInternodes",
                        choices=valid_tasks, metavar='task',
                        help=f"Task to evaluate, should be in: {', '.join(valid_tasks)}")
    parser.add_argument("config_file", type=str,
                        help="Path to the pipeline TOML configuration file.")
    parser.add_argument("-n", "--replicate_number", default=30, type=int,
                        help="Number of replicate to use for repeatability evaluation. Defaults to `30`.")
    parser.add_argument("-s", "--suffix", default="", type=str,
                        help="Suffix to append to the created database folder.")
    parser.add_argument("-f", "--full_pipe", action="store_true", default=False,
                        help="Run the whole Reconstruction & Quantification pipeline on each replicate independently.")
    parser.add_argument("-np", "--no_pipeline", action="store_true", default=False,
                        help="Do not run the pipeline, only compare tasks outputs.")
    parser.add_argument("-db", '--eval_database', type=str, default="",
                        help="Evaluation database location to use. Use at your own risks!")
    parser.add_argument("--models", default=False,
                        help="Models database location to use with ML pipeline.")
    parser.add_argument('--log-level', type=str, dest='log_level', default='INFO',
                        choices=list(logging._nameToLevel.keys()),
                        help="Set message logging level. Defaults to `INFO`.")

    # ->Evaluation database path exists:
    #   - NO: create the directory & use it instead of auto-generated path
    #   - YES:
    #     ->Evaluation database contain scan datasets
    #       - NO: use it instead of auto-generated path
    #       - YES: use number of scans as replicate number & compute repeatability evaluation

    return parser


def file_by_file_comparison(db, task_name, scan_replicates):
    """Compares task folders, file by file, dump results to JSON.

    Parameters
    ----------
    db : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        name of the task to evaluation
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.

    """
    from itertools import combinations
    logger.info(f"Performing file-by-file comparison for the outputs of task '{task_name}'...")
    fcmp = {}
    scan_pairs = combinations(scan_replicates, 2)
    for ref_scan, flo_scan in scan_pairs:
        # Compare the files' `os.stat()` signatures (file type, size, and modification time):
        cmp = filecmp.dircmp(ref_scan, flo_scan)
        # Compute the replicated task folders similarity percentage:
        n_diff = len(cmp.diff_files)
        n_same = len(cmp.same_files)
        if n_same == 0:
            similarity = 0
        else:
            similarity = (n_same / (n_diff + n_same)) * 100
        fcmp[f"{ref_scan.name} - {flo_scan.name}"] = similarity

    average_similarity = average_pairwise_comparison(fcmp)
    # Log result to terminal:
    if len(scan_replicates) <= 3:
        logger.info(f"File-by-file comparison (similarity percentage) between pairs of repetitions:{fcmp}")
        logger.info(f"Mean Similarity: {average_similarity:.2f} %")

    # Write a JSON summary file of the comparison:
    with open(Path(db.basedir) / 'filebyfile_comparison.json', 'w') as out_file:
        json.dump({'Similarity (%)': fcmp, 'Mean similarity (%)': average_similarity}, out_file)
    return


def compare_task_output(db, task_name, scan_replicates, independent_runs=False):
    """Method to compare tasks outputs on a series of replicated datasets.

    Parameters
    ----------
    db : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        Name of the task to evaluation.
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.
    independent_runs : bool
        ``True`` indicate that the chained tasks were performed independently for each replicate.
        So each of them (all tasks downstream of `task_name` & `task_name`) will be evaluated.
        Else only perform the evaluation defined for `task_name`.

    Notes
    -----
    The behaviour of this method is configured by the JSON file `CONF_FILE`.
    It is possible to define more than one comparison method per task.

    """
    db.connect()
    # List paths where tested task output(s) should be located:
    valid_replicates = []
    for scan in scan_replicates:
        try:
            _ = _get_task_fileset(db.get_scan(scan.name), task_name).id
        except AssertionError as e:
            logger.error(e)
        else:
            valid_replicates.append(scan)

    n_actual_replicates = len(valid_replicates)
    logger.info(f"Got {n_actual_replicates} valid replicates of task '{task_name}' (with an output fileset).")

    # - Performs file-by-file comparisons for the selected task:
    file_by_file_comparison(db, task_name, valid_replicates)
    # - Load the metric for the selected task from the JSON configuration file:
    comparison_config = json.load(open(CONF_FILE))

    def _perform_comparison(db, comparison_config, task_name, valid_replicates):
        # Get the list of `plantdb.fsdb.Scan` instances to compare:
        valid_replicates = [db.get_scan(scan.name) for scan in valid_replicates]
        # - Get the comparison function(s) from the `robustness_evaluation` JSON configuration file
        comp_func = comparison_config[task_name]["comp_func"]
        if isinstance(comp_func, str):
            comp_func = [comp_func]  # make a list to work in following loop
        # - More than one comparison function can be applied to a same task!
        for func in comp_func:
            logger.info(f"Comparing '{task_name}' outputs with '{func}'...")
            eval(func + "(db, task_name, valid_replicates)")
            # try:
            #     eval(func + "(db, task_name)")
            # except NameError:
            #     logger.warning(f"The comparison function `{func}` "
            #                    "(defined in robustness_evaluation.json) does NOT exist!")
            # except Exception as e:
            #     logger.error(f"The comparison function `{func}` failed!")
            #     logger.error(e)

    # - Evaluate all tasks if independent tests, else evaluate only tested task:
    if independent_runs:
        for task in comparison_config.keys():
            _perform_comparison(db, comparison_config, task, valid_replicates)
    else:
        _perform_comparison(db, comparison_config, task_name, valid_replicates)


def fill_eval_db(eval_db, init_scan, config, previous_task, nb, models_path=None, extra_scans=None):
    """Fill the evaluation DB with replicates of `init_scan` and run the pipeline up to the previous task.

    Parameters
    ----------
    eval_db : pathlib.Path or str
        Path to the evaluation DB.
    init_scan : pathlib.Path or str
        Path to the scan dataset to use for evaluation.
    config : pathlib.Path or str
        Path to the configuration file to use to run the pipeline (``romi_run_task``).
    previous_task : str
        Name of the task to execute prior to the task to evaluation.
    nb : int
        Number time to duplicatd the inital scan dataset.
    models_path : pathlib.Path
        Name of models directory to copy in evaluation DB, to use with task ``Segmentation2D``.
    extra_scans : list of pathlib.Path
        Name of other scan dataset directories to copy in evaluation DB.

    Returns
    -------
    list
        list of pathlib.Path of created scan datasets

    Notes
    -----
    The evaluation database is created as follows:
      - create a temporary directory (`/tmp/tmp_{init_scan.name}`)
      - copy the 'models' dataset, if any
      - copy the extra scan dataset, if any
      - copy the initial scan dataset (`init_scan`) to the temporary directory
      - clean
      - runs the pipeline up to the `previous_task` using the `config`, if any
      - replicate the previous scan directory `nb` times to the evaluation DB (`eval_db`)

    """
    scan_replicates = []
    # Initialize a temporary directory use to store scan dataset before cleaning and running previous tasks to task to analyse:
    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)
        # - Get the path of the temporary ROMI database
        tmp_scan_folder = tmp_path / f"tmp_{init_scan.name}"
        logger.info(f"Create a temporary directory '{tmp_scan_folder}'.")
        # - Create the `MARKER_FILE_NAME` marker for an active `plantdb.fsdb.FSDB`
        logger.info(f"Create the `{MARKER_FILE_NAME}` marker for an active `plantdb.fsdb.FSDB`.")
        with open(tmp_path / MARKER_FILE_NAME, 'w'):
            pass

        # - If ML models are needed, copy them...
        if models_path is not None:
            logger.info("Duplicate the ML models datasets...")
            # ...to temporary directory
            dest = tmp_path / "models"
            try:
                shutil.copytree(str(models_path), str(dest))
            except:
                logger.error(f"Failed to copy '{models_path}' to '{dest}'!")
            # ...to final directory
            dest = eval_db / "models"
            try:
                shutil.copytree(str(models_path), str(dest))
            except:
                logger.error(f"Failed to copy '{models_path}' to '{dest}'!")

        logger.info("Duplicate the extra scan dataset...")
        # - If extra scans are needed, copy them...
        if extra_scans is not None:
            for x_scan in extra_scans:
                # ...to temporary directory
                dest = tmp_path / x_scan.name
                try:
                    shutil.copytree(str(x_scan), str(dest))
                except:
                    logger.error(f"Failed to copy '{x_scan}' to '{dest}'!")
                # ...to final directory
                dest = eval_db / x_scan.name
                try:
                    shutil.copytree(str(x_scan), str(dest))
                except:
                    logger.error(f"Failed to copy '{x_scan}' to '{dest}'!")

        logger.info("Duplicate the 'initial scan dataset'...")
        # Duplicate the initial scan dataset to the temporary folder
        try:
            shutil.copytree(str(init_scan), str(tmp_scan_folder))
        except:
            logger.error(f"Failed to copy '{init_scan}' to '{tmp_scan_folder}'!")

        # Check for previous analysis & if yes, clean the dataset
        init_configuration_file = tmp_scan_folder / "pipeline.toml"
        if init_configuration_file.is_file():
            logger.info("Using detected configuration pipeline backup file to clean the scan dataset!")
            print(str(init_configuration_file))
            romi_run_task(tmp_scan_folder, "Clean", str(init_configuration_file))

        # Run the previous task on the temporary copy, if any:
        if previous_task != "":
            logger.info(f"Running {previous_task} pipeline on the scan dataset!")
            romi_run_task(tmp_scan_folder, previous_task, config)

        # Duplicate the results of the previous task to the evaluation database:
        if previous_task != "":
            logger.info(f"Copying previous run of {previous_task} pipeline...")
        else:
            logger.info(f"Duplicating temporary copy of the 'initial scan dataset'...")
        for i in range(nb):
            logger.info(f"Copy #{i}...")
            copied_scan_name = Path(eval_db / f"{init_scan.name}_{i}")
            shutil.copytree(str(tmp_scan_folder), str(copied_scan_name))
            scan_replicates.append(copied_scan_name.expanduser())

    return scan_replicates


def get_eval_db_path(root_location, task_name, suffix=""):
    """Generate the "evaluation database" name, for example: '20200803124841_eval_TriangleMesh'.

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str, optional
        Suffix to append to the created database folder.

    Returns
    -------
    pathlib.Path
        Path to the evaluation database.
    """
    now = datetime.datetime.now()
    now_str = now.strftime("%Y%m%d%H%M%S")
    folder_name = f"{now_str}_eval_{task_name}"
    if suffix != "":
        if not suffix.startswith("_"):
            suffix = f"_{suffix}"
        folder_name += suffix
    return Path(root_location / folder_name)


def initialize_eval_db(test_db="", **kwargs):
    """Initialize the local FSDB "evaluation database".

    Parameters
    ----------
    test_db : str or pathlib.Path
        Full path to the evaluation database directory.

    Other Parameters
    ----------------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str
        Suffix to append to the created database folder.

    Returns
    -------
    pathlib.Path
        Path to the "evaluation database" directory.

    Notes
    -----
    A valid ROMI database (`plantdb`) is a directory with a `romidb` marker file in it.
    We use the tested task name and datetime to create the name of the directory.

    See Also
    --------
    plant3dvision.bin.robustness_evaluation.create_test_db_path

    """
    # - Get the path to the evaluation DB to create:
    if test_db == "":
        test_db = get_eval_db_path(kwargs.get('root_location'), kwargs.get('task_name'), kwargs.get('suffix'))
    else:
        test_db = Path(test_db)
    # Create the evaluation DB directory:
    test_db.mkdir(exist_ok=True)
    # - Creates the marker file to be able to get a `plantdb.fsdb.FSDB` instance:
    marker_file = test_db / MARKER_FILE_NAME
    marker_file.touch()
    return test_db


def get_replicates_list(eval_db, init_scan):
    """Get the list of scan replicates.

    Parameters
    ----------
    eval_db : pathlib.Path
        Full path to evaluation database.
    init_scan : pathlib.Path
        Full path to scan dataset to use for evaluation.

    Returns
    -------
    list of str
        List of full path to the replicates of the initial scan found in the evaluation database.

    Examples
    --------
    >>> from pathlib import Path
    >>> eval_db = Path('~/romi_db/calibration_robustness/20220905155852_eval_ExtrinsicCalibration_opencv')
    >>> init_scan = Path('~/romi_db/calibration_robustness/db/extrinsic_calib_1')
    >>> get_replicates_list(eval_db.expanduser(), init_scan.expanduser())

    """
    return sorted(
        list([idir.expanduser() for idir in eval_db.iterdir() if idir.is_dir() and init_scan.name in idir.name]))


def romi_run_task(scan_path, task_name, cfg_file):
    """Run configured pipeline for given task on a scan.

    Parameters
    ----------
    scan_path : pathlib.Path
       Path to the scan dataset directory.
    task_name : str
        Name of the ROMI task to run.
    cfg_file : str
        Path to the configuration file to use to run the pipeline (``romi_run_task``).

    """
    logger.info(f"Executing task '{task_name}' on scan dataset '{str(scan_path)}'.")
    # TODO: use luigi.build() instead of subprocess.run call ?
    cmd = ["romi_run_task", task_name, str(scan_path), "--config", cfg_file, "--local-scheduler"]
    subprocess.run(cmd, check=True)
    return


def main():
    # - Parse the input arguments to variables:
    parser = parsing()
    args = parser.parse_args()

    # - Get the path to the initial scan dataset:
    init_scan_path = Path(args.scan).expanduser()
    # - Path to the local database that contain the initial scan dataset:
    db_location = init_scan_path.parent
    # - Path to the local database
    root_location = db_location.parent
    # - Load the JSON config file `robustness_evaluation.json`:
    config = json.load(open(CONF_FILE))
    # - Get the task to execute prior to the one to evaluate, if any, from the JSON evaluation configuration file:
    prev_task = config[args.task]["prev_task"]

    # - Initialize the "evaluation database" and get its path:
    eval_db_path = initialize_eval_db(args.eval_database, task_name=args.task, root_location=root_location,
                                      suffix=args.suffix)

    # - Initialize the logger
    global logger
    logger = configure_logger(filename, eval_db_path, args.log_level)
    logger.info(f"Path to the scan dataset is: {init_scan_path}")
    if args.eval_database != "":
        logger.info(f"Got a manually defined evaluation database location: '{args.eval_database}'.")

    # - Search pipeline config for '*calibration_scan_id':
    pipeline_cfg = toml.load(open(args.config_file, 'r'))
    extra_scans = []
    for section, cfg_dict in pipeline_cfg.items():
        for k, v in cfg_dict.items():
            if "calibration_scan_id" in k and v != "":
                extra_scans.append(db_location / v)
    nx = len(extra_scans)
    if nx != 0:
        extra_scans_str = ', '.join([xs.name for xs in extra_scans])
        logger.info(f"Found '{nx}' calibration scan{'s' if nx > 1 else ''} to copy: {extra_scans_str}")

    # - Get the path to the CNN model
    if args.models:
        models_path = Path(args.models).expanduser()
    else:
        models_path = None

    # - Scan evaluation DB to check if it's already populated with replicates:
    scan_replicates = get_replicates_list(eval_db_path, init_scan_path)
    n_replicates = len(scan_replicates)
    # Case without/with scan datasets in the evaluation database root folder
    if n_replicates == 0:
        # Duplicate the scan dataset as many times as requested
        n_replicates = int(args.replicate_number)
        scan_replicates = fill_eval_db(eval_db_path, init_scan_path, args.config_file, prev_task,
                                       n_replicates, models_path, extra_scans)
    else:
        # If there is something in the root folder that mean you passed a value to `--eval_database`
        # And we then consider that it already contains the replicated scans!
        logger.info(f"Found {n_replicates} scan replicates in existing evaluation database!")

    # - Make sure the `romidb` marker file exists:
    marker_file = eval_db_path / MARKER_FILE_NAME
    try:
        marker_file.touch()
    except:
        pass
    # - Make sure the `lock` file do NOT exist:
    lock_file = eval_db_path / LOCK_FILE_NAME
    try:
        lock_file.unlink()
        # lock_file.unlink(missing_ok=True)  # missing_ok only available since Python3.8
    except:
        pass

    # - Instantiate `FSDB` local "evaluation database":
    test_db = FSDB(str(eval_db_path))

    # - Execute the task to evaluate for all replicated datasets:
    if not args.no_pipeline:
        logger.info(f"Repeating task '{args.task}' on {n_replicates} replicates of scan dataset '{init_scan_path}'...")
        for replicated_scan in tqdm(scan_replicates, unit='replicate'):
            romi_run_task(replicated_scan, args.task, args.config_file)

    # - Compare the output(s) of the task(s) between replicated datasets:
    compare_task_output(test_db, args.task, scan_replicates, independent_runs=args.full_pipe)


if __name__ == '__main__':
    main()
