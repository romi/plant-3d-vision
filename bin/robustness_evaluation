#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import datetime
import logging
import os
import shutil
import subprocess
import tempfile

import toml
from plant3dvision.compare import *
from plant3dvision.compare import _get_task_fileset
from plantdb import FSDB
from plantdb.fsdb import LOCK_FILE_NAME
from plantdb.fsdb import MARKER_FILE_NAME
from romitask.log import configure_logger
from tqdm import tqdm

dirname, filename = os.path.split(os.path.abspath(__file__))

CONF_FILE = os.path.join(dirname, "robustness_evaluation.json")
URL = "https://docs.romi-project.eu/plant_imager/developer/pipeline_repeatability/"

DESC = """Robustness evaluation of the Reconstruction & Quantification pipelines.

Evaluating the repeatability of a Reconstruction & Quantification (R&Q) pipeline is made as follows:
 1. duplicate the selected scan dataset in a temporary directory (and clean it from previous R&Q if necessary)
 2. run the R&Q pipeline up to the previous task of the selected task to evaluate, if any
 3. copy/replicate this result to a new database (append a replicate id to the dataset name)
 4. run the task to evaluate for each replicated dataset
 5. compare the directories of the task to evaluate pair by pair
 6. apply the comparison metrics for the task to evaluate, as defined in `robustness_evaluation.json` 

Please note that:
 - Directory comparisons are done at the scale of the files generated by the selected task.
 - We use metrics to get a quantitative comparison on the output of the task.
 - It is possible to create fully independent repetitions by running the whole R&Q pipeline using `-f`.
 - In order to use the ML-based R&Q pipeline, you will have to:
   1. create an output directory
   2. use the `--models` argument to copy the CNN trained models
"""

DATETIME_FMT = "%Y.%m.%d_%H.%M"

def parsing():
    # TODO: an option to set a reference (or would that be the job of the Evaluation tasks?!)
    # TODO: use the `pipeline.toml` to defines the previous tasks ?
    # - Load the JSON config file of the script:
    config = json.load(open(CONF_FILE))
    valid_tasks = sorted(set(config.keys()) - set(['Clean']))

    parser = argparse.ArgumentParser(description=DESC,
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     epilog=f"Detailed explanations here: {URL}")

    parser.add_argument("ref_scan", type=str,
                        help="Reference scan dataset to use for repeatability evaluation.")
    parser.add_argument("task", type=str, default="AnglesAndInternodes",
                        choices=valid_tasks, metavar='task',
                        help=f"Task to evaluate, should be in: {', '.join(valid_tasks)}")
    parser.add_argument("config_file", type=str,
                        help="Path to the pipeline TOML configuration file.")

    # -- Evaluation options:
    eval = parser.add_argument_group('Evaluation options')
    eval.add_argument("-n", "--replicate_number", default=30, type=int,
                        help="Number of replicate to use for repeatability evaluation. Defaults to `30`.")
    eval.add_argument("-c", "--clean", action="store_true", default=False,
                        help="Force Clean task on scan dataset.")
    eval.add_argument("-f", "--full-pipe", dest='full_pipe', action="store_true", default=False,
                        help="Run the whole Reconstruction & Quantification pipeline on each replicate independently.")
    eval.add_argument("-np", "--no-pipeline", dest='no_pipeline', action="store_true", default=False,
                        help="Do not run the pipeline, only compare tasks outputs.")
    eval.add_argument("--models", default=False,
                        help="Models database location to use with ML pipeline.")

    # -- Evaluation database options:
    db = parser.add_argument_group('Database options')
    db.add_argument("--suffix", default="", type=str,
                        help="Suffix to append to the created database directory.")
    db.add_argument('--eval-db', dest='eval_db', type=str, default="",
                        help="Evaluation database location to use. Use at your own risks!")
    db.add_argument('--date-fmt', dest='date_fmt', type=str, default=DATETIME_FMT,
                        help="Datetime format to use prefix for evaluation database name. Defaults to `DATETIME_FMT`.")
    db.add_argument('--no-date', dest='no_date', action="store_true", default=False,
                        help=f"Do not add the datetime as prefix.")

    parser.add_argument('--log-level', dest='log_level', type=str, default='INFO',
                        choices=list(logging._nameToLevel.keys()),
                        help="Set message logging level. Defaults to `INFO`.")

    # ->Evaluation database path exists:
    #   - NO: create the directory & use it instead of auto-generated path
    #   - YES:
    #     ->Evaluation database contain scan datasets
    #       - NO: use it instead of auto-generated path
    #       - YES: use number of scans as replicate number & compute repeatability evaluation

    return parser


def file_by_file_comparison(db, task_name, scan_replicates):
    """Compares task folders, file by file, dump results to JSON.

    Parameters
    ----------
    db : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        name of the task to evaluation
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.

    Notes
    -----
    Produce a JSON file that summarize the comparison.

    See Also
    --------
    filecmp.dircmp

    """
    import filecmp
    from itertools import combinations
    logger.info(f"Performing file-by-file comparison for the outputs of task '{task_name}'...")
    fcmp = {}
    scan_pairs = combinations(scan_replicates, 2)
    for ref_scan, flo_scan in scan_pairs:
        # Compare the files' `os.stat()` signatures (file type, size, and modification time):
        cmp = filecmp.dircmp(ref_scan, flo_scan)
        # Compute the replicated task folders similarity percentage:
        n_diff = len(cmp.diff_files)
        n_same = len(cmp.same_files)
        if n_same == 0:
            similarity = 0
        else:
            similarity = (n_same / (n_diff + n_same)) * 100
        fcmp[f"{ref_scan.name} - {flo_scan.name}"] = similarity

    average_similarity = average_pairwise_comparison(fcmp)
    # Log result to terminal:
    if len(scan_replicates) <= 3:
        logger.info(f"File-by-file comparison (similarity percentage) between pairs of repetitions:{fcmp}")
        logger.info(f"Mean Similarity: {average_similarity:.2f} %")

    # Write a JSON summary file of the comparison:
    with open(Path(db.basedir) / 'filebyfile_comparison.json', 'w') as out_file:
        json.dump({'Similarity (%)': fcmp, 'Mean similarity (%)': average_similarity}, out_file)
    return


def compare_task_output(db, task_name, scan_replicates, independent_runs=False):
    """Method to compare tasks outputs on a series of replicated datasets.

    Parameters
    ----------
    db : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        Name of the task to evaluation.
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.
    independent_runs : bool
        ``True`` indicate that the chained tasks were performed independently for each replicate.
        So each of them (all tasks downstream of `task_name` & `task_name`) will be evaluated.
        Else only perform the evaluation defined for `task_name`.

    Notes
    -----
    The behaviour of this method is configured by the JSON file `CONF_FILE`.
    It is possible to define more than one comparison method per task.

    """
    db.connect()
    # - Check the list of scan replicates for successful call of `romi_run_task`
    # (scan dataset should contain the `task_name` output fileset):
    valid_replicates = []
    for scan in scan_replicates:
        try:
            _ = _get_task_fileset(db.get_scan(scan.name), task_name).id
        except AssertionError as e:
            logger.error(e)
        else:
            valid_replicates.append(scan)
    n_actual_replicates = len(valid_replicates)
    logger.info(f"Got {n_actual_replicates} valid replicates of task '{task_name}' (with an output fileset).")

    # - Performs file-by-file comparisons for the selected task:
    file_by_file_comparison(db, task_name, valid_replicates)
    # - Load the metric for the selected task from the JSON configuration file:
    comparison_config = json.load(open(CONF_FILE))

    # - Evaluate all tasks if independent tests, else evaluate only tested task:
    if independent_runs:
        for task in comparison_config.keys():
            comp_func = comparison_config[task]["comp_func"]
            # If no comparison function is defined, skip this!
            if comp_func == "":
                continue
            # Check the task has been called by pipeline (should produce output fileset), otherwise continue the loop
            try:
                _ = _get_task_fileset(db.get_scan(valid_replicates[0].name), task)
            except AssertionError:
                continue
            _perform_comparisons(db, comp_func, task, valid_replicates)
    else:
        comp_func = comparison_config[task_name]["comp_func"]
        _perform_comparisons(db, comp_func, task_name, valid_replicates)

    return


def _perform_comparisons(db, comp_func, task_name, valid_replicates):
    """Method that calls for the task comparison function(s) on the list of scan replicates that successfully ran it."""
    if isinstance(comp_func, str):
        comp_func = [comp_func]  # make a list to work in following loop
    # Get the list of `plantdb.fsdb.Scan` instances to compare from list of Path:
    valid_replicates = [db.get_scan(scan.name) for scan in valid_replicates]
    # - More than one comparison function can be applied to a same task!
    for func in comp_func:
        logger.info(f"Comparing '{task_name}' outputs with '{func}'...")
        eval(func + "(db, task_name, valid_replicates)")
        # try:
        #     eval(func + "(db, task_name)")
        # except NameError:
        #     logger.warning(f"The comparison function `{func}` "
        #                    "(defined in robustness_evaluation.json) does NOT exist!")
        # except Exception as e:
        #     logger.error(f"The comparison function `{func}` failed!")
        #     logger.error(e)
    return


def _copy_tree(origin, dest):
    """Copy a whole directory from `origin` to `dest`."""
    try:
        shutil.copytree(str(origin), str(dest))
    except:
        logger.error(f"Failed to copy '{origin}' to '{dest}'!")
    return


def fill_tmp_db(ref_scan, models_path=None, extra_scans=None, clean=False):
    """Fill the temporary DB with replicates of `ref_scan` and run the pipeline up to the previous task.

    Parameters
    ----------
    ref_scan : pathlib.Path or str
        Path to the 'reference scan dataset' to use for evaluation.
    models_path : pathlib.Path
        Name of models directory to copy in evaluation DB, to use with task ``Segmentation2D``.
    extra_scans : list of pathlib.Path
        Name of other scan dataset directories to copy in evaluation DB.
    clean : bool, optional
        Force cleaning the 'reference scan dataset' to use for evaluation.

    Returns
    -------
    Path
        The path to the temporary 'reference scan dataset'.

    Notes
    -----
    The evaluation database is created as follows:
      - create a temporary directory (`/tmp/tmp_{ref_scan.name}`)
      - copy the 'models' dataset, if any
      - copy the extra scan dataset, if any
      - copy the reference scan dataset (`ref_scan`) to the temporary directory

    """
    # Initialize a temporary directory use to store scan dataset before cleaning and running previous tasks to task to analyse:
    tmp_dir = tempfile.mkdtemp()
    # - Get the path of the temporary ROMI database
    tmp_path = Path(tmp_dir)
    logger.info(f"Create a temporary directory '{tmp_path}'.")
    # - Create the `MARKER_FILE_NAME` marker for an active `plantdb.fsdb.FSDB`
    with open(tmp_path / MARKER_FILE_NAME, 'w'):
        logger.info(f"Create the `{MARKER_FILE_NAME}` marker for an active `plantdb.fsdb.FSDB`.")

    # - If ML models are needed, copy them to evaluation DB directory:
    if models_path is not None:
        logger.info("Copy the required ML models dataset...")
        _copy_tree(models_path, tmp_path / "models")
    # - If extra scans are needed, copy them to evaluation DB directory:
    if extra_scans is not None:
        for x_scan in extra_scans:
            logger.info(f"Copy the required calibration dataset '{x_scan.name}'...")
            _copy_tree(x_scan, tmp_path / x_scan.name)

    # Copy the 'reference scan dataset' to the temporary directory
    logger.info("Copy the 'reference scan dataset' to the temporary directory...")
    tmp_scan_dir = tmp_path / f"{ref_scan.name}"
    _copy_tree(ref_scan, tmp_scan_dir)

    _check_clean(tmp_scan_dir, clean)

    return tmp_scan_dir


def _check_clean(tmp_scan_dir, clean):
    """Check for previous analysis & if yes, clean the dataset, can be forced by `clean`."""
    bak_cfg_file = tmp_scan_dir / "pipeline.toml"
    if bak_cfg_file.is_file():
        romi_run_task(tmp_scan_dir, "Clean", str(bak_cfg_file))
    elif clean:
        # Create a Clean task configuration:
        with open(bak_cfg_file, 'w') as cfg_f:
            cfg = {"Clean": {"no_confirm": True}}
            toml.dump(cfg, cfg_f)
        romi_run_task(tmp_scan_dir, "Clean", str(bak_cfg_file))
    else:
        logger.info("No cleaning of the reference scan dataset!")
    return


def fill_eval_db(eval_db, ref_scan, nb, models_path=None, extra_scans=None):
    """Fill the evaluation DB with replicates of `ref_scan` and run the pipeline up to the previous task.

    Parameters
    ----------
    eval_db : pathlib.Path
        Path to the evaluation DB.
    ref_scan : pathlib.Path
        Path to the scan dataset to use for evaluation.
    nb : int
        Number time to duplicate the reference scan dataset.
    models_path : pathlib.Path
        Name of models directory to copy in evaluation DB, to use with task ``Segmentation2D``.
    extra_scans : list of pathlib.Path
        Name of other scan dataset directories to copy in evaluation DB.

    Returns
    -------
    list
        list of pathlib.Path of created scan datasets

    Notes
    -----
    The evaluation database is created as follows:
      - create a temporary directory (`/tmp/tmp_{ref_scan.name}`)
      - copy the 'models' dataset, if any
      - copy the extra scan dataset, if any
      - copy the reference scan dataset (`ref_scan`) to the temporary directory
      - clean
      - runs the pipeline up to the `previous_task` using the `config`, if any
      - replicate the previous scan directory `nb` times to the evaluation DB (`eval_db`)

    """
    scan_replicates = []
    # - If ML models are needed, copy them to evaluation DB directory:
    if models_path is not None:
        logger.info("Copy the required ML models dataset...")
        _copy_tree(models_path, eval_db / "models")
    # - If extra scans are needed, copy them to evaluation DB directory:
    if extra_scans is not None:
        for x_scan in extra_scans:
            logger.info(f"Copy the required calibration dataset '{x_scan.name}'...")
            _copy_tree(x_scan, eval_db / x_scan.name)

    for i in range(nb):
        copied_scan_name = eval_db / f"{ref_scan.name}_{i}"
        shutil.copytree(str(ref_scan), str(copied_scan_name))
        scan_replicates.append(copied_scan_name.expanduser())

    return scan_replicates


def get_eval_db_path(root_location, task_name, suffix="", date_fmt=DATETIME_FMT, no_date=False, **kwargs):
    """Generate the "evaluation database" name, for example: '20200803124841_eval_TriangleMesh'.

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str, optional
        Suffix to append to the created database directory.
    date_fmt : str, optional
        Datetime format to append as evaluation database prefix. Defaults to ``DATETIME_FMT``.
    no_date : bool, optional
        Do not add the datetime as prefix. Defaults to ``False``.

    Returns
    -------
    pathlib.Path
        Path to the evaluation database.
    """
    if no_date:
        folder_name = f"Eval_{task_name}"
    else:
        now = datetime.datetime.now()
        now_str = now.strftime(date_fmt)
        folder_name = f"{now_str}_Eval_{task_name}"

    if suffix != "":
        if not suffix.startswith("_"):
            suffix = f"_{suffix}"
        folder_name += suffix

    return Path(root_location / folder_name)


def initialize_eval_db(test_db="", **kwargs):
    """Initialize the local FSDB "evaluation database".

    Parameters
    ----------
    test_db : str or pathlib.Path
        Full path to the evaluation database directory.

    Other Parameters
    ----------------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str
        Suffix to append to the created database directory.
    date_fmt : str
        Datetime format to append as evaluation database prefix. Defaults to ``DATETIME_FMT``.
    no_date : bool
        Do not add the datetime as prefix. Defaults to ``False``.

    Returns
    -------
    pathlib.Path
        Path to the "evaluation database" directory.

    Notes
    -----
    A valid ROMI database (`plantdb`) is a directory with a `romidb` marker file in it.
    We use the tested task name and datetime to create the name of the directory.

    See Also
    --------
    plant3dvision.bin.robustness_evaluation.create_test_db_path

    """
    # - Get the path to the evaluation DB to create:
    if test_db == "":
        test_db = get_eval_db_path(**kwargs)
    else:
        test_db = Path(test_db)
    # Create the evaluation DB directory:
    test_db.mkdir(exist_ok=True)
    # - Creates the marker file to be able to get a `plantdb.fsdb.FSDB` instance:
    marker_file = test_db / MARKER_FILE_NAME
    marker_file.touch()
    return test_db


def get_replicates_list(eval_db, ref_scan):
    """Get the list of scan replicates.

    Parameters
    ----------
    eval_db : pathlib.Path
        Full path to evaluation database.
    ref_scan : pathlib.Path
        Full path to scan dataset to use for evaluation.

    Returns
    -------
    list of str
        List of full path to the replicates of the reference scan found in the evaluation database.

    Examples
    --------
    >>> from pathlib import Path
    >>> eval_db = Path('~/romi_db/calibration_robustness/20220905155852_eval_ExtrinsicCalibration_opencv')
    >>> ref_scan = Path('~/romi_db/calibration_robustness/db/extrinsic_calib_1')
    >>> get_replicates_list(eval_db.expanduser(), ref_scan.expanduser())

    """
    return sorted(
        list([idir.expanduser() for idir in eval_db.iterdir() if idir.is_dir() and ref_scan.name in idir.name]))


def romi_run_task(scan_path, task_name, cfg_file):
    """Run configured pipeline for given task on a scan.

    Parameters
    ----------
    scan_path : pathlib.Path
       Path to the scan dataset directory.
    task_name : str
        Name of the ROMI task to run.
    cfg_file : str
        Path to the configuration file to use to run the pipeline (``romi_run_task``).

    """
    logger.info(f"Executing task '{task_name}' on scan dataset '{str(scan_path)}'.")
    # TODO: use luigi.build() instead of subprocess.run call ?
    cmd = ["romi_run_task", task_name, str(scan_path), "--config", cfg_file, "--local-scheduler"]
    subprocess.run(cmd, check=True)
    return


def _check_markers(path):
    # - Make sure the `romidb` marker file exists:
    marker_file = path / MARKER_FILE_NAME
    try:
        marker_file.touch()
    except:
        pass
    # - Make sure the `lock` file do NOT exist:
    lock_file = path / LOCK_FILE_NAME
    try:
        lock_file.unlink()
        # lock_file.unlink(missing_ok=True)  # missing_ok only available since Python3.8
    except:
        pass
    return


def check_extra_scans(db_location, config_file):
    """Use the configuration file to search for defined calibration scan id(s) to copy."""
    pipeline_cfg = toml.load(open(config_file, 'r'))
    extra_scans = []
    for section, cfg_dict in pipeline_cfg.items():
        for k, v in cfg_dict.items():
            if "calibration_scan_id" in k and v != "":
                extra_scans.append(db_location / v)
    nx = len(extra_scans)
    if nx != 0:
        extra_scans_str = ', '.join([xs.name for xs in extra_scans])
        logger.info(f"Found '{nx}' calibration scan{'s' if nx > 1 else ''} to copy: {extra_scans_str}")
    return extra_scans


def main():
    # - Parse the input arguments to variables:
    parser = parsing()
    args = parser.parse_args()

    # - Get the path to the reference scan dataset:
    ref_scan_path = Path(args.ref_scan).expanduser()
    # - Path to the local database that contain the reference scan dataset:
    db_location = ref_scan_path.parent
    # - Load the JSON config file `robustness_evaluation.json`:
    config = json.load(open(CONF_FILE))
    # - Get the task to execute prior to the one to evaluate, if any, from the JSON evaluation configuration file:
    if args.full_pipe:
        prev_task = ""  # we want independent replicates, so no common previous task!
    else:
        prev_task = config[args.task]["prev_task"]

    # - Initialize the "evaluation database" and get its path:
    eval_db_path = initialize_eval_db(args.eval_db, task_name=args.task, root_location=db_location.parent,
                                      suffix=args.suffix, date_fmt=args.date_fmt, no_date=args.no_date)

    if len(list(eval_db_path.iterdir())) > 1 and args.eval_db == "":
        # We did not specify an eval DB but the directory is not empty:
        from plant3dvision.utils import yes_no_choice
        empty_dir = yes_no_choice(f"The directory '{eval_db_path}' is not EMPTY! Do you want to clean it?")
        if empty_dir:
            # Remove the existing eval DB:
            shutil.rmtree(eval_db_path)
            # Re-initialize the "evaluation database" and get its path:
            eval_db_path = initialize_eval_db(args.eval_db, task_name=args.task, root_location=db_location.parent,
                                              suffix=args.suffix, date_fmt=args.date_fmt, no_date=args.no_date)

    # - Initialize the logger
    global logger
    logger = configure_logger(filename, eval_db_path, args.log_level)
    logger.info(f"Path to the 'reference scan dataset' is: '{ref_scan_path}'")
    if args.eval_db != "":
        logger.info(f"Got a manually defined evaluation database location: '{args.eval_db}'.")

    # - Search pipeline config for '*calibration_scan_id':
    extra_scans = check_extra_scans(db_location, args.config_file)
    # - Get the path to the CNN model
    models_path = Path(args.models).expanduser() if args.models else None

    # - Scan evaluation DB to check if it's already populated with replicates:
    scan_replicates = get_replicates_list(eval_db_path, ref_scan_path)
    n_replicates = len(scan_replicates)
    # Case without/with scan datasets in the evaluation database root directory
    if n_replicates == 0:
        n_replicates = int(args.replicate_number)
        # If there is a previous task to run:
        if prev_task != "":
            # Create & populate the temporary DB:
            ref_scan_path = fill_tmp_db(ref_scan_path, models_path, extra_scans, clean=args.clean)
            # Run the previous task on the temporary DB, if any:
            logger.info(f"Call to '{prev_task}' task on the 'temporary scan dataset'...")
            romi_run_task(ref_scan_path, prev_task, config)
            # Duplicate the results of the previous task to the evaluation database:
            logger.info(f"Duplicate the 'temporary scan dataset' {n_replicates} times...")
        else:
            bak_cfg_file = ref_scan_path / "pipeline.toml"
            # If a previous analysis is found (directory has a `pipeline.toml`) or a clean was requested:
            if bak_cfg_file.is_file() or args.clean:
                # Copy to a temporary DB, clean it, replace reference scan path by clean one
                ref_scan_path = fill_tmp_db(ref_scan_path, clean=args.clean)
                logger.info(f"Duplicate the 'temporary scan dataset' {n_replicates} times...")
            else:
                logger.info(f"Duplicate the 'reference scan dataset' {n_replicates} times...")
        # Duplicate the scan dataset as many times as requested
        scan_replicates = fill_eval_db(eval_db_path, ref_scan_path, n_replicates, models_path, extra_scans)
    else:
        # If there is something in the root directory that mean you passed a value to `--eval_db`
        # And we thus consider that it should already contain the replicated scans!
        logger.info(f"Found {n_replicates} replicate scans in evaluation database!")

    # Clean the temporary DB:
    tmp_db = ref_scan_path.parent
    if tmp_db.is_dir():
        os.rmdir(ref_scan_path.parent)

    # - Instantiate `FSDB` local "evaluation database":
    _check_markers(eval_db_path)
    test_db = FSDB(str(eval_db_path))

    # - Execute the task to evaluate for all replicated datasets:
    if not args.no_pipeline:
        if prev_task == "":
            logger.info(f"Independent calls to task '{args.task}' on {n_replicates} replicate scans...")
        else:
            logger.info(f"Repeat calls to task '{args.task}' on {n_replicates} replicate scans...")
        for replicated_scan in tqdm(scan_replicates, unit='replicate'):
            romi_run_task(replicated_scan, args.task, args.config_file)

    # - Compare the output(s) of the task(s) between replicated datasets:
    compare_task_output(test_db, args.task, scan_replicates, independent_runs=args.full_pipe)


if __name__ == '__main__':
    main()
