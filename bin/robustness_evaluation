#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import datetime
import glob
import logging
import os
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path

import toml
from tqdm import tqdm

from plant3dvision.compare import *
from plant3dvision.compare import _get_task_fileset
from plantdb import FSDB
from plantdb.fsdb import LOCK_FILE_NAME
from plantdb.fsdb import MARKER_FILE_NAME
from romitask.log import configure_logger

dirname, filename = os.path.split(os.path.abspath(__file__))

CONF_FILE = os.path.join(dirname, "robustness_evaluation.json")
URL = "https://docs.romi-project.eu/plant_imager/developer/pipeline_repeatability/"

DESC = """Task robustness evaluation.

Evaluating the robustness of a task from the Reconstruction & Quantification (R&Q) pipeline is made as follows:
 1. copy the selected scan dataset in a temporary database (and clean it from previous R&Q if necessary)
 2. run the R&Q pipeline up to the upstream task of the task to evaluate, if any upstream task exist
 3. replicate (copy) this result to an evaluation database as many times as requested (by the `-n` option, defaults to `30`)
 4. run the task to evaluate on each replicated scan dataset
 5. compare the directories and files of the task to evaluate pair by pair
 6. use the comparison metrics for the task to evaluate, as defined in `robustness_evaluation.json` 

Please note that:
 - at step 3, a _replicate id_ is appended to the replicated scan dataset name
 - directory comparisons are done at the scale of the files generated by the selected task.
 - we use metrics to get a quantitative comparison on the output of the task.
 - it is possible to create fully independent repetitions by running the whole R&Q pipeline on each scan dataset using the `--full-pipe` option (or the shorter `-f`).
 - in order to use the ML-based R&Q pipeline, you will have to:
   1. create an output directory
   2. use the `--models` argument to copy the CNN trained models
"""

DATETIME_FMT = "%Y.%m.%d_%H.%M"


def parsing():
    # TODO: an option to set a reference (or would that be the job of the Evaluation tasks?!)
    # TODO: use the `pipeline.toml` to defines the previous tasks ?
    # - Load the JSON config file of the script:
    config = json.load(open(CONF_FILE))
    valid_tasks = sorted(set(config.keys()) - {'Clean'})

    parser = argparse.ArgumentParser(description=DESC,
                                     formatter_class=argparse.RawDescriptionHelpFormatter,
                                     epilog=f"Detailed explanations here: {URL}")

    parser.add_argument("task", type=str, default="AnglesAndInternodes",
                        choices=valid_tasks, metavar='task',
                        help=f"Task to evaluate, should be in: {', '.join(valid_tasks)}")
    parser.add_argument("dataset_path", type=str, default='', nargs='*',
                        help="Path to scan dataset to use for task robustness evaluation.")
    parser.add_argument('--config', dest='config', type=str, default="",
                        help="Path to the pipeline TOML configuration file.")

    # -- Evaluation options:
    eval = parser.add_argument_group('Evaluation options')
    eval.add_argument("-n", "--n_replicates", default=30, type=int,
                      help="Number of replicates to create for task robustness evaluation. "
                           "Defaults to `30`.")
    eval.add_argument("-c", "--clean", action="store_true", default=False,
                      help="Run a Clean task on scan dataset prior to duplication.")
    eval.add_argument("-f", "--full-pipe", dest='full_pipe', action="store_true", default=False,
                      help="Use this to run the whole pipeline independently for each replicate. "
                           "Else the task to evaluate is run on clones of the results from the upstream task, if any.")
    eval.add_argument("-np", "--no-pipeline", dest='no_pipeline', action="store_true", default=False,
                      help="Do not run the pipeline, only compare tasks outputs. "
                           "Use with `--eval-db` to rerun this code on an existing test evaluation database!")

    # -- Evaluation database options:
    db = parser.add_argument_group('Database options')
    db.add_argument("--suffix", default="", type=str,
                    help="Suffix to append to the evaluation database directory to create.")
    db.add_argument('--eval-db', dest='eval_db', type=str, default="",
                    help="Existing evaluation database location to use. "
                         "Use with `-np` to rerun this code on an existing test evaluation database!")
    db.add_argument('--date-fmt', dest='date_fmt', type=str, default=DATETIME_FMT,
                    help="Datetime format to use as prefix for the name of the evaluation database directory to create. "
                         "Defaults to `DATETIME_FMT`.")
    db.add_argument('--no-date', dest='no_date', action="store_true", default=False,
                    help=f"Do not add the datetime as prefix to the name of the evaluation database directory to create.")

    others = parser.add_argument_group('Other options')
    others.add_argument("--models", default=False,
                      help="Models database location to use with ML pipeline.")
    others.add_argument('--log-level', dest='log_level', type=str, default='INFO',
                        choices=list(logging._nameToLevel.keys()),
                        help="Set message logging level. Defaults to `INFO`.")

    # ->Evaluation database path exists:
    #   - NO: create the directory & use it instead of auto-generated path
    #   - YES:
    #     ->Evaluation database contain scan datasets
    #       - NO: use it instead of auto-generated path
    #       - YES: use number of scans as replicate number & evaluate task robustness

    return parser


def file_by_file_comparison(eval_db_path, task_name, scan_replicates):
    """Compares task folders, file by file, dump results to JSON.

    Parameters
    ----------
    eval_db_path : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        name of the task to evaluation
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.

    Notes
    -----
    Produce a JSON file that summarize the comparison.

    See Also
    --------
    filecmp.dircmp

    """
    import filecmp
    from itertools import combinations
    logger.info(f"Performing file-by-file comparison for the outputs of task '{task_name}'...")
    fcmp = {}
    scan_pairs = combinations(scan_replicates, 2)
    for ref_scan, flo_scan in scan_pairs:
        # Compare the files' `os.stat()` signatures (file type, size, and modification time):
        cmp = filecmp.dircmp(ref_scan, flo_scan)
        # Compute the replicated task folders similarity percentage:
        n_diff = len(cmp.diff_files)
        n_same = len(cmp.same_files)
        if n_same == 0:
            similarity = 0
        else:
            similarity = (n_same / (n_diff + n_same)) * 100
        fcmp[f"{ref_scan.name} - {flo_scan.name}"] = similarity

    average_similarity = average_pairwise_comparison(fcmp)
    # Log result to terminal:
    if len(scan_replicates) <= 3:
        logger.info(f"File-by-file comparison (similarity percentage) between pairs of repetitions:{fcmp}")
        logger.info(f"Mean Similarity: {average_similarity:.2f} %")

    # Write a JSON summary file of the comparison:
    with open(Path(eval_db_path.basedir) / 'filebyfile_comparison.json', 'w') as out_file:
        json.dump({'Similarity (%)': fcmp, 'Mean similarity (%)': average_similarity}, out_file)
    return


def compare_task_output(eval_db_path, task_name, scan_replicates, independent_runs=False):
    """Method to compare tasks outputs on a series of replicated datasets.

    Parameters
    ----------
    eval_db_path : plantdb.fsdb.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : str
        Name of the task to evaluation.
    scan_replicates : list of pathlib.Path
        List of `Path` to the replicated scans.
    independent_runs : bool
        ``True`` indicate that the chained tasks were performed independently for each replicate.
        So each of them (all tasks downstream of `task_name` & `task_name`) will be evaluated.
        Else only perform the evaluation defined for `task_name`.

    Notes
    -----
    The behaviour of this method is configured by the JSON file `CONF_FILE`.
    It is possible to define more than one comparison method per task.

    """
    eval_db_path.connect()
    # - Check the list of scan replicates for successful call of `romi_run_task`
    # (scan dataset should contain the `task_name` output fileset):
    valid_replicates = []
    for scan in scan_replicates:
        try:
            _ = _get_task_fileset(eval_db_path.get_scan(scan.name), task_name).id
        except AssertionError as e:
            logger.error(e)
        else:
            valid_replicates.append(scan)
    n_actual_replicates = len(valid_replicates)
    logger.info(f"Got {n_actual_replicates} valid replicates of task '{task_name}' (with an output fileset).")

    # - Performs file-by-file comparisons for the selected task:
    file_by_file_comparison(eval_db_path, task_name, valid_replicates)
    # - Load the metric for the selected task from the JSON configuration file:
    comparison_config = json.load(open(CONF_FILE))

    # - Evaluate all tasks if independent tests, else evaluate only tested task:
    if independent_runs:
        for task in comparison_config.keys():
            comp_func = comparison_config[task]["comp_func"]
            # If no comparison function is defined, skip this!
            if comp_func == "":
                continue
            # Check the task has been called by pipeline (should produce output fileset), otherwise continue the loop
            try:
                _ = _get_task_fileset(eval_db_path.get_scan(valid_replicates[0].name), task)
            except AssertionError:
                continue
            _perform_comparisons(eval_db_path, comp_func, task, valid_replicates)
    else:
        comp_func = comparison_config[task_name]["comp_func"]
        _perform_comparisons(eval_db_path, comp_func, task_name, valid_replicates)

    return


def _perform_comparisons(eval_db_path, comp_func, task_name, valid_replicates):
    """Method that calls for the task comparison function(s) on the list of scan replicates that successfully ran it."""
    if isinstance(comp_func, str):
        comp_func = [comp_func]  # make a list to work in following loop
    # Get the list of `plantdb.fsdb.Scan` instances to compare from list of Path:
    valid_replicates = [eval_db_path.get_scan(scan.name) for scan in valid_replicates]
    # - More than one comparison function can be applied to a same task!
    for func in comp_func:
        logger.info(f"Comparing '{task_name}' outputs with '{func}'...")
        eval(func + "(eval_db_path, task_name, valid_replicates)")
        # try:
        #     eval(func + "(db, task_name)")
        # except NameError:
        #     logger.warning(f"The comparison function `{func}` "
        #                    "(defined in robustness_evaluation.json) does NOT exist!")
        # except Exception as e:
        #     logger.error(f"The comparison function `{func}` failed!")
        #     logger.error(e)
    return


def _copy_tree(origin, dest):
    """Copy a whole directory from `origin` to `dest`."""
    try:
        shutil.copytree(str(origin), str(dest))
    except:
        logger.error(f"Failed to copy '{origin}' to '{dest}'!")
    return


def fill_tmp_db(dataset_path, models_path=None, extra_scans=None, clean=False):
    """Fill the temporary DB with replicates of `ref_scan` and run the pipeline up to the previous task.

    Parameters
    ----------
    dataset_path : pathlib.Path or str
        Path to the 'reference scan dataset' to use for evaluation.
    models_path : pathlib.Path
        Name of models directory to copy in evaluation DB, to use with task ``Segmentation2D``.
    extra_scans : list of pathlib.Path
        Name of other scan dataset directories to copy in evaluation DB.
    clean : bool, optional
        Force cleaning the 'reference scan dataset' to use for evaluation.

    Returns
    -------
    Path
        The path to the temporary 'reference scan dataset'.

    Notes
    -----
    The evaluation database is created as follows:
      - create a temporary directory (`/tmp/tmp_{ref_scan.name}`)
      - copy the 'models' dataset, if any
      - copy the extra scan dataset, if any
      - copy the reference scan dataset (`ref_scan`) to the temporary directory

    """
    # Initialize a temporary directory use to store scan dataset before cleaning and running previous tasks to task to analyse:
    tmp_dir = tempfile.mkdtemp()
    # - Get the path of the temporary ROMI database
    tmp_path = Path(tmp_dir)
    logger.info(f"Create a temporary directory '{tmp_path}'.")
    # - Create the `MARKER_FILE_NAME` marker for an active `plantdb.fsdb.FSDB`
    with open(tmp_path / MARKER_FILE_NAME, 'w'):
        logger.info(f"Create the `{MARKER_FILE_NAME}` marker for an active `plantdb.fsdb.FSDB`.")

    # - If ML models are needed, copy them to evaluation DB directory:
    if models_path is not None:
        logger.info("Copy the required ML models dataset...")
        _copy_tree(models_path, tmp_path / "models")
    # - If extra scans are needed, copy them to evaluation DB directory:
    if extra_scans is not None:
        for x_scan in extra_scans:
            logger.info(f"Copy the required calibration dataset '{x_scan.name}'...")
            _copy_tree(x_scan, tmp_path / x_scan.name)

    # Copy the 'reference scan dataset' to the temporary directory
    logger.info("Copy the 'reference scan dataset' to the temporary directory...")
    tmp_scan_dir = tmp_path / f"{dataset_path.name}"
    _copy_tree(dataset_path, tmp_scan_dir)

    _run_clean_if_requested(tmp_scan_dir, clean)

    return tmp_scan_dir


def _run_clean_if_requested(tmp_scan_dir, clean):
    """Clean the input dataset if requested."""
    bak_cfg_file = tmp_scan_dir / "pipeline.toml"
    if clean:
        # Create a quiet Clean task configuration:
        with open(bak_cfg_file, 'w') as cfg_f:
            cfg = {"Clean": {"no_confirm": True}}
            toml.dump(cfg, cfg_f)
        romi_run_task(tmp_scan_dir, "Clean", str(bak_cfg_file))
    else:
        logger.info("No cleaning of the reference scan dataset!")
    return


def fill_eval_db(eval_db_path, dataset_path, n_replicate, models_path=None, extra_scans=None):
    """Fill the evaluation DB with replicates of `ref_scan` and run the pipeline up to the previous task.

    Parameters
    ----------
    eval_db_path : pathlib.Path
        The path to the evaluation DB.
    dataset_path : pathlib.Path
        The path to the dataset to use for evaluation.
    n_replicate : int
        Number time to duplicate the reference dataset.
    models_path : pathlib.Path
        Name of models directory to copy in evaluation DB, to use with task ``Segmentation2D``.
    extra_scans : list of pathlib.Path
        Name of other dataset directories to copy in evaluation DB.

    Returns
    -------
    list
        list of pathlib.Path of created scan datasets

    Notes
    -----
    The evaluation database is created as follows:
      - create a temporary directory (`/tmp/tmp_{ref_scan.name}`)
      - copy the 'models' dataset, if any
      - copy the extra scan dataset, if any
      - copy the reference scan dataset (`ref_scan`) to the temporary directory
      - clean
      - runs the pipeline up to the `previous_task` using the `config`, if any
      - replicate the previous scan directory `nb` times to the evaluation DB (`eval_db`)

    """
    scan_replicates = []
    # - If ML models are needed, copy them to evaluation DB directory:
    if models_path is not None:
        logger.info("Copy the required ML models dataset...")
        _copy_tree(models_path, eval_db_path / "models")
    # - If extra scans are needed, copy them to evaluation DB directory:
    if extra_scans is not None:
        for x_scan in extra_scans:
            logger.info(f"Copy the required calibration dataset '{x_scan.name}'...")
            _copy_tree(x_scan, eval_db_path / x_scan.name)

    n_fill = len(str(n_replicate))
    for i in range(n_replicate):
        copied_scan_name = eval_db_path / f"{dataset_path.name}_{str(i).zfill(n_fill)}"
        shutil.copytree(str(dataset_path), str(copied_scan_name))
        scan_replicates.append(copied_scan_name.expanduser())

    return scan_replicates


def get_eval_db_path(root_location, task_name, suffix="", date_fmt=DATETIME_FMT, no_date=False, **kwargs):
    """Generate the "evaluation database" name, for example: '20200803124841_eval_TriangleMesh'.

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str, optional
        Suffix to append to the created database directory.
    date_fmt : str, optional
        Datetime format to append as evaluation database prefix. Defaults to ``DATETIME_FMT``.
    no_date : bool, optional
        Do not add the datetime as prefix. Defaults to ``False``.

    Returns
    -------
    pathlib.Path
        Path to the evaluation database.
    """
    if no_date:
        folder_name = f"Eval_{task_name}"
    else:
        now = datetime.datetime.now()
        now_str = now.strftime(date_fmt)
        folder_name = f"{now_str}_Eval_{task_name}"

    if suffix != "":
        if not suffix.startswith("_"):
            suffix = f"_{suffix}"
        folder_name += suffix

    return Path(root_location / folder_name)


def initialize_eval_db(eval_db_path="", **kwargs):
    """Initialize the local FSDB "evaluation database".

    Parameters
    ----------
    eval_db_path : str or pathlib.Path
        Full path to the evaluation database directory.

    Other Parameters
    ----------------
    root_location : pathlib.Path
        Root path where to create the evaluation database.
    task_name : str
        Name of the task to evaluate.
    suffix : str
        Suffix to append to the created database directory.
    date_fmt : str
        Datetime format to append as evaluation database prefix. Defaults to ``DATETIME_FMT``.
    no_date : bool
        Do not add the datetime as prefix. Defaults to ``False``.

    Returns
    -------
    pathlib.Path
        Path to the "evaluation database" directory.

    Notes
    -----
    A valid ROMI database (`plantdb`) is a directory with a `romidb` marker file in it.
    We use the tested task name and datetime to create the name of the directory.

    See Also
    --------
    plant3dvision.bin.robustness_evaluation.create_test_db_path

    """
    # - Get the path to the evaluation DB to create:
    if eval_db_path == "":
        eval_db_path = get_eval_db_path(**kwargs).absolute()
    else:
        eval_db_path = Path(eval_db_path).absolute()
    # Create the evaluation DB directory:
    eval_db_path.mkdir(exist_ok=True)
    # - Creates the marker file to be able to get a `plantdb.fsdb.FSDB` instance:
    marker_file = eval_db_path / MARKER_FILE_NAME
    marker_file.touch()
    return eval_db_path


def get_replicates_list(eval_db_path, ds_path):
    """Get the list of scan replicates.

    Parameters
    ----------
    eval_db_path : pathlib.Path
        Full path to evaluation database.
    ds_path : pathlib.Path
        Full path to scan dataset to use for evaluation.

    Returns
    -------
    list of str
        List of full path to the replicates of the reference scan found in the evaluation database.

    Examples
    --------
    >>> from pathlib import Path
    >>> eval_db_path = Path('~/romi_db/calibration_robustness/20220905155852_eval_ExtrinsicCalibration_opencv')
    >>> ds_path = Path('~/romi_db/calibration_robustness/db/extrinsic_calib_1')
    >>> get_replicates_list(eval_db_path.expanduser(),ds_path.expanduser())

    """
    return sorted(
        list([idir.expanduser() for idir in eval_db_path.iterdir() if idir.is_dir() and ds_path.name in idir.name]))


def romi_run_task(dataset_path, task_name, cfg_file):
    """Run configured pipeline for given task on a scan.

    Parameters
    ----------
    dataset_path : pathlib.Path
       Path to the scan dataset directory.
    task_name : str
        Name of the ROMI task to run.
    cfg_file : str
        Path to the configuration file to use to run the pipeline (``romi_run_task``).

    """
    logger.info(f"Executing task '{task_name}' on scan dataset '{str(dataset_path)}'.")
    # TODO: use luigi.build() instead of subprocess.run call ?
    cmd = ["romi_run_task", task_name, str(dataset_path), "--config", cfg_file, "--local-scheduler"]
    logger.info(cmd)
    subprocess.run(cmd, check=True)
    return


def _check_markers(path):
    # - Make sure the `romidb` marker file exists:
    marker_file = path / MARKER_FILE_NAME
    try:
        marker_file.touch()
    except:
        pass
    # - Make sure the `lock` file do NOT exist:
    lock_file = path / LOCK_FILE_NAME
    try:
        lock_file.unlink()
        # lock_file.unlink(missing_ok=True)  # missing_ok only available since Python3.8
    except:
        pass
    return


def check_extra_dataset(db_location, config_file):
    """Use the configuration file to search for defined calibration dataset id(s) to copy.

    Parameters
    ----------
    db_location : pathlib.Path
        The path to the database hosting the input dataset used for robustness evaluation.
    config_file : pathlib.Path
        The path to the pipeline configuration file.

    Returns
    -------
    list
        The list of extra dataset required to run the robustness evaluation with this pipeline configuration.
    """
    pipeline_cfg = toml.load(open(config_file, 'r'))
    extra_ds = []
    for section, cfg_dict in pipeline_cfg.items():
        for k, v in cfg_dict.items():
            if "calibration_scan_id" in k and v != "":
                extra_ds.append(db_location / v)
    nx = len(extra_ds)
    if nx != 0:
        extra_scans_str = ', '.join([xs.name for xs in extra_ds])
        logger.info(f"Found '{nx}' calibration scan{'s' if nx > 1 else ''} to copy: {extra_scans_str}")
    return extra_ds


def run_task_eval(args):
    """Run the robustness evaluation of a task on a dataset."""
    # - Get the path to the reference scan dataset:
    dataset_path = Path(args.dataset_path).expanduser()
    # - Path to the local database that contain the reference scan dataset:
    db_location = dataset_path.parent
    # - Load the JSON config file `robustness_evaluation.json`:
    config = json.load(open(CONF_FILE))
    # - Get the task to execute prior to the one to evaluate, if any, from the JSON evaluation configuration file:
    if args.full_pipe:
        prev_task = ""  # we want independent replicates, so no common previous task!
    else:
        prev_task = config[args.task]["prev_task"]

    # - Initialize the "evaluation database" and get its path:
    eval_db_path = initialize_eval_db(args.eval_db, task_name=args.task, root_location=db_location.parent,
                                      suffix=args.suffix, date_fmt=args.date_fmt, no_date=args.no_date)

    if len(list(eval_db_path.iterdir())) > 1 and args.eval_db == "":
        # We did not specify an eval DB but the directory is not empty:
        from plant3dvision.utils import yes_no_choice
        empty_dir = yes_no_choice(f"The directory '{eval_db_path}' is not EMPTY! Do you want to clean it?")
        if empty_dir:
            # Remove the existing eval DB:
            shutil.rmtree(eval_db_path)
            # Re-initialize the "evaluation database" and get its path:
            eval_db_path = initialize_eval_db(args.eval_db, task_name=args.task, root_location=db_location.parent,
                                              suffix=args.suffix, date_fmt=args.date_fmt, no_date=args.no_date)

    # - Initialize the logger
    global logger
    logger = configure_logger(filename, eval_db_path, args.log_level)
    logger.info(f"Robustness evaluation of task {args.task} is running on dataset '{Path(args.dataset_path).name}'...")
    logger.info(f"Path to the 'input scan dataset' is: '{dataset_path}'")
    # TODO: check if the scan is "raw" or not using call to some Clean task methods to get insight?
    if args.eval_db != "":
        logger.info(f"Got a manually defined evaluation database location: '{args.eval_db}'.")
    else:
        logger.info(f"Created an evaluation database at location: '{eval_db_path}'.")

    # - Check if some extra dataset are required (search in pipeline config, e.g. for '*calibration_scan_id'):
    extra_dataset = check_extra_dataset(db_location, args.config)
    # - Get the path to the CNN model
    path2cnn_model = Path(args.models).expanduser() if args.models else None

    # - Scan evaluation DB to check if it's already populated with replicates:
    scan_replicates = get_replicates_list(eval_db_path, dataset_path)
    n_replicates = len(scan_replicates)
    has_tmp_db = False
    # Case without/with scan datasets in the evaluation database root directory
    if n_replicates == 0:
        n_replicates = int(args.n_replicates)
        # If there is a previous task to run:
        if prev_task != "":
            # Create & populate the temporary DB:
            dataset_path = fill_tmp_db(dataset_path, path2cnn_model, extra_dataset, clean=args.clean)
            has_tmp_db = True
            # Run the previous task on the temporary DB, if any:
            logger.info(f"Call to '{prev_task}' task on the 'temporary scan dataset'...")
            romi_run_task(dataset_path, prev_task, args.config)
            # Duplicate the results of the previous task to the evaluation database:
            logger.info(f"Duplicate the 'temporary scan dataset' {n_replicates} times...")
        else:
            bak_cfg_file = dataset_path / "pipeline.toml"
            # If a previous analysis is found (directory has a `pipeline.toml`) or a clean was requested:
            if bak_cfg_file.is_file() or args.clean:
                # Copy to a temporary DB, clean it, replace reference scan path by clean one
                dataset_path = fill_tmp_db(dataset_path, clean=args.clean)
                has_tmp_db = True
                logger.info(f"Duplicate the 'temporary scan dataset' {n_replicates} times...")
            else:
                logger.info(f"Duplicate the 'reference scan dataset' {n_replicates} times...")
        # Duplicate the scan dataset as many times as requested
        scan_replicates = fill_eval_db(eval_db_path, dataset_path, n_replicates, path2cnn_model, extra_dataset)
    else:
        # If there is something in the root directory that mean you gave a value to `--eval_db`
        # And we thus consider that it should already contain the replicated scans!
        logger.info(f"Found {n_replicates} replicate scans in evaluation database!")
        if n_replicates != int(args.n_replicates):
            logger.warning(
                f"Not the same number of replicates found ({n_replicates}) and requested ({int(args.n_replicates)})!")

    # Clean the temporary DB:
    tmp_db = dataset_path.parent
    if has_tmp_db and tmp_db.is_dir():
        shutil.rmtree(dataset_path.parent)

    # - Instantiate `FSDB` local "evaluation database":
    _check_markers(eval_db_path)
    test_db = FSDB(str(eval_db_path))

    # - Execute the task to evaluate for all replicated datasets:
    if not args.no_pipeline:
        # -- Set the `PYOPENCL_CTX` environment variable:
        if os.getenv('PYOPENCL_CTX') == None:
            os.environ["PYOPENCL_CTX"] = '0'

        if prev_task == "":
            logger.info(f"Independent calls to task '{args.task}' on {n_replicates} replicate scans...")
        else:
            logger.info(f"Repeat calls to task '{args.task}' on {n_replicates} replicate scans...")
        for replicated_scan in tqdm(scan_replicates, unit='replicate'):
            romi_run_task(replicated_scan, args.task, args.config)

    # - Compare the output(s) of the task(s) between replicated datasets:
    compare_task_output(test_db, args.task, scan_replicates, independent_runs=args.full_pipe)


def main():
    # - Parse the input arguments to variables:
    parser = parsing()
    args = parser.parse_args()

    # - If only one path in the list, get the first one:
    if len(args.dataset_path) == 1:
        args.dataset_path = args.dataset_path[0]

    if isinstance(args.dataset_path, str):
        # Process the input string `args.dataset_path` with ``glob``:
        #   - check existence of path:
        #   - may contain UNIX matching symbols (like '*' or '?'):
        folders = glob.glob(args.dataset_path)
        # Resolve path (make it absolute & normalize):
        folders = [Path(path).resolve() for path in folders]
        # Check that globed paths are directory (and exist, implied):
        folders = sorted([path for path in folders if path.is_dir()])
    elif isinstance(args.dataset_path, list):
        # Resolve path (make it absolute & normalize):
        folders = [Path(path).resolve() for path in args.dataset_path]
        # Check that listed paths are directory (and exist, implied):
        folders = sorted([path for path in folders if path.is_dir()])
    else:
        sys.exit(f"Error with input dataset path for '{args.task}' module!")

    print(f"Found {len(folders)} folders!")

    for folder in folders:
        args.dataset_path = folder
        print("\n")  # to facilitate the search in the console by separating the datasets
        try:
            run_task_eval(args)
        except Exception as e:
            print(e)


if __name__ == '__main__':
    main()
