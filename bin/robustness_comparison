#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
import datetime
import filecmp
import json
import logging
import os
import pathlib
import shutil
import subprocess
import tempfile
from collections import Counter
from os.path import splitext

import matplotlib.pyplot as plt
import numpy as np
import open3d as o3d
from romidata import FSDB
from romidata.fsdb import LOCK_FILE_NAME
from romidata.fsdb import MARKER_FILE_NAME
from romidata.io import read_image
from romidata.io import read_json
from romidata.io import read_point_cloud
from romidata.io import read_triangle_mesh
from romiscan.metrics import chamfer_distance
from romiscan.metrics import point_cloud_registration_fitness
from romiscan.metrics import set_metrics
from romiscan.metrics import surface_ratio
from romiscan.metrics import volume_ratio

dirname, filename = os.path.split(os.path.abspath(__file__))
logger = logging.getLogger(f'{filename}')

CONF_FILE = os.path.join(dirname, "conf_robustness_comparison.json")


def save_data_repartition(data, data_type, db):
    """Save repartition plots.

    Parameters
    ----------
    data : list
        list of data to plot
    data_type : string
        angles or internodes
    db : pathlib.Path
        folder in which register the graphs

    """
    fig, ax = plt.subplots()
    ax.set_title(f'{data_type} distribution for same scan same pipe')
    ax.boxplot(list(np.array(data).transpose()))
    fig.savefig(db / f"repeat_results_{data_type}.png")


def _get_task_fileset(scan_dataset, task_name):
    """Returns the `Fileset` object produced by `task_name` in given `scan_dataset`.

    Parameters
    ----------
    scan_dataset : romidata.FSDB.Scan
        scan dataset that should contain the unique output of `task_name`
    task_name : string
        name of the task to test

    Returns
    -------
    romidata.FSDB.Scan.Fileset
        The `Fileset` object produced by `task_name` in given `scan_dataset`

    Raises
    ------
    AssertionError
        If there is more than one `Fileset` object matching the task.
        This mean that this task was run several times with different parameters.

    """
    task_filesets = [fs for fs in scan_dataset.filesets if fs.id.startswith(task_name)]
    try:
        assert len(task_filesets) == 1
    except AssertionError:
        msg = f"There is more than one occurrence of the task {task_name}:\n"
        msg += "  - " + "  - ".join([tfs.id for tfs in task_filesets])
        raise AssertionError(msg)
    return task_filesets[0]


def _get_files(scan_dataset, task_name, unique=False):
    """Returns the `File` object produced by `task_name` in given `scan_dataset`.

    Parameters
    ----------
    scan_dataset : romidata.FSDB.Scan
        scan dataset that should contain the unique output of `task_name`
    task_name : string
        name of the task to test
    unique : bool, optional
        if ``True``, assert that there is only one `File` in the `Fileset`

    Returns
    -------
    list(romidata.FSDB.Scan.Fileset.File)
        The `File` objects produced by `task_name` in given `scan_dataset`

    Raises
    ------
    AssertionError
        If there is more than one `File` object in the task `Fileset`, requires ``unique=True``.

    """
    # Get the `Fileset` corresponding to the replicated task:
    fs = _get_task_fileset(scan_dataset, task_name)
    # Get the list of `File` ids (task outputs)
    f_list = [f.id for f in fs.files]
    if unique:
        try:
            assert len(f_list) == 1
        except AssertionError:
            raise AssertionError("This method can only compare tasks that output a single point-cloud!")
    # - Return the `File` objects:
    return [fs.get_file(f) for f in f_list]


def compare_binary_mask(db, task_name):
    """Use set metrics to compare binary masks for each unique pairs of repetition.

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    precision, recall = {}, {}
    mean_precision, mean_recall = {}, {}
    median_precision, median_recall = {}, {}
    # List the duplicated `Scan` datasets:
    scans_list = [scan for scan in db.get_scans()]
    logger.info(f"Comparing {task_name} output for replicated scans list: {[s.id for s in scans_list]}")
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_scan in enumerate(scans_list):
        # Get `Fileset` with mask files:
        ref_fileset = _get_task_fileset(ref_scan, task_name)
        if t + 1 > len(scans_list):
            break  # all unique pairs to compare are done! 
        for flo_scan in scans_list[t + 1:]:
            # Get `Fileset` with mask files:
            flo_fileset = _get_task_fileset(flo_scan, task_name)
            # Compare the two mask files list to get the list of 'same files" (ie. with same name)
            ref_dir = pathlib.Path(db.basedir, ref_scan.id, ref_fileset.id)
            flo_dir = pathlib.Path(db.basedir, flo_scan.id, flo_fileset.id)
            common_files = filecmp.dircmp(ref_dir, flo_dir).common_files
            # Remove the extension as the `Fileset.get_file()` method does not accept it!
            common_files = [splitext(sf)[0] for sf in common_files]
            # Dictionary key made of the two repeat names:
            k = f"{ref_scan.id} - {flo_scan.id}"
            precision[k], recall[k] = {}, {}
            # For each file with the same name compute some set metrics:
            for sf in common_files:
                # Read the two mask files:
                ref_mask = read_image(ref_fileset.get_file(sf))
                flo_mask = read_image(flo_fileset.get_file(sf))
                # Compute set metrics for each pair of masks:
                _, _, _, _, p, r = set_metrics(ref_mask, flo_mask)
                precision[k][sf] = p
                recall[k][sf] = r
            # Compute an average & the median for the metrics:
            mean_precision[k] = np.mean(list(precision[k].values()))
            mean_recall[k] = np.mean(list(recall[k].values()))
            median_precision[k] = np.median(list(precision[k].values()))
            median_recall[k] = np.median(list(recall[k].values()))

    print("\nAverage precision (closer to 1.0 is better) between pairs of repetitions:")
    print(mean_precision)
    print("\nAverage recall (closer to 1.0 is better) between pairs of repetitions:")
    print(mean_recall)
    print("\nMedian precision (closer to 1.0 is better) between pairs of repetitions:")
    print(median_precision)
    print("\nMedian recall (closer to 1.0 is better) between pairs of repetitions:")
    print(median_recall)

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / f'{task_name}_comparison.json', 'w') as out_file:
        json.dump({
            'mean precision': mean_precision, 'mean recall': mean_recall,
            'median precision': median_precision, 'median recall': median_recall,
            'precision': precision, 'recall': recall
        }, out_file, indent=2)

    return


def compare_pointcloud(db, task_name):
    """Use the chamfer distance to compare point-cloud for each unique pairs of repetition.

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    chamfer_dist = {}
    fitness = {}
    # List the duplicated `Scan` datasets:
    scans_list = [scan for scan in db.get_scans()]
    logger.info(f"Comparing {task_name} output for replicated scans list: {[s.id for s in scans_list]}")
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_scan in enumerate(scans_list):
        # - Read the `PointCloud.ply` file
        ref_pcd_file = _get_files(ref_scan, task_name, unique=True)[0]
        ref_pcd = read_point_cloud(ref_pcd_file)
        if t + 1 > len(scans_list):
            break  # all unique pairs to compare are done! 
        for flo_scan in scans_list[t + 1:]:
            # - Read the `PointCloud.ply` file
            flo_pcd_file = _get_files(flo_scan, task_name, unique=True)[0]
            flo_pcd = read_point_cloud(flo_pcd_file)
            k = f"{ref_scan.id} - {flo_scan.id}"
            # - Compute the Chamfer distance
            chamfer_dist[k] = chamfer_distance(ref_pcd, flo_pcd)
            # - Compute the registration fitness & residuals
            fitness[k] = point_cloud_registration_fitness(ref_pcd, flo_pcd)

    # Print result to terminal:
    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    print(chamfer_dist)
    # Print result to terminal:
    print("\nFitness (closer to 1.0 is better) & residuals (inliner RMSE) between pairs of repetitions:")
    print(fitness)

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / f'{task_name}_comparison.json', 'w') as out_file:
        json.dump({'chamfer distances': chamfer_dist, 'fitness & residuals': fitness}, out_file)

    return


def compare_trianglemesh_points(db, task_name):
    """Use the chamfer distance to compare mesh vertices point-cloud for each unique pairs of repetition.

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    chamfer_dist = {}
    surf_ratio = {}
    vol_ratio = {}
    # List the duplicated `Scan` datasets:
    scans_list = [scan for scan in db.get_scans()]
    logger.info(f"Comparing {task_name} output for replicated scans list: {[s.id for s in scans_list]}")
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_scan in enumerate(scans_list):
        # - Read the `TriangleMesh.ply` file
        ref_mesh_file = _get_files(ref_scan, task_name, unique=True)[0]
        ref_mesh = read_triangle_mesh(ref_mesh_file)
        # - Extract a PointCloud from the mesh vertices
        ref_pcd = o3d.geometry.PointCloud(ref_mesh.vertices)
        if t + 1 > len(scans_list):
            break  # all unique pairs to compare are done! 
        for flo_scan in scans_list[t + 1:]:
            # - Read the `TriangleMesh.ply` file
            flo_mesh_file = _get_files(flo_scan, task_name, unique=True)[0]
            flo_mesh = read_triangle_mesh(flo_mesh_file)
            # - Extract a PointCloud from the mesh vertices
            flo_pcd = o3d.geometry.PointCloud(flo_mesh.vertices)
            k = f"{ref_scan.id} - {flo_scan.id}"
            # - Compute the Chamfer distance
            chamfer_dist[k] = chamfer_distance(ref_pcd, flo_pcd)
            surf_ratio[k] = surface_ratio(ref_mesh, flo_mesh)
            vol_ratio[k] = volume_ratio(ref_mesh, flo_mesh)

    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    print(chamfer_dist)
    print("\nSurface ratio (closer to 1.0 is better) between pairs of repetitions:")
    print(surf_ratio)
    print("\nVolume ratio (closer to 1.0 is better) between pairs of repetitions:")
    print(vol_ratio)

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / f'{task_name}_comparison.json', 'w') as out_file:
        json.dump({
            'chamfer distances': chamfer_dist,
            'surface ratio': surf_ratio,
            'volume ratio': vol_ratio
        }, out_file)

    return


def compare_curveskeleton_points(db, task_name):
    """Use the chamfer distance to compare point-cloud for each unique pairs of repetition.

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    chamfer_dist = {}
    # List the duplicated `Scan` datasets:
    scans_list = [scan for scan in db.get_scans()]
    logger.info(f"Comparing {task_name} output for replicated scans list: {[s.id for s in scans_list]}")
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_scan in enumerate(scans_list):
        # - Read the `CurveSkeleton.json` file
        ref_json_file = _get_files(ref_scan, task_name, unique=True)[0]
        ref_json = read_json(ref_json_file)
        # - Extract a PointCloud from the skeleton vertices
        ref_pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(ref_json["points"]))
        if t + 1 > len(scans_list):
            break  # all unique pairs to compare are done!
        for flo_scan in scans_list[t + 1:]:
            # - Read the `CurveSkeleton.json` file
            flo_json_file = _get_files(flo_scan, task_name, unique=True)[0]
            flo_json = read_json(flo_json_file)
            # - Extract a PointCloud from the skeleton vertices
            flo_pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(flo_json["points"]))
            k = f"{ref_scan.id} - {flo_scan.id}"
            # - Compute the Chamfer distance
            chamfer_dist[k] = chamfer_distance(ref_pcd, flo_pcd)

    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    print(chamfer_dist)

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / f'{task_name}_comparison.json', 'w') as out_file:
        json.dump({'chamfer distances': chamfer_dist}, out_file)

    return


def angles_and_internodes_comparison(db, task_name):
    """Coarse comparison of sequences of angles and internodes.

    Prints a dict containing the number of scans for each number of organs indexed, ex : {12:4, 14:6} (4 scans with 12
    organs recognized and 6 with 14 organs)
    For the biggest number of scans with same number of organs, creates plots of the repartition of angles (and
    internodes) for each organ id

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    angles_and_internodes = {}
    nb_organs = []
    # List the duplicated `Scan` datasets:
    scans_list = [scan for scan in db.get_scans()]
    logger.info(f"Comparing {task_name} output for replicated scans list: {[s.id for s in scans_list]}")
    # - Load all JSON files with angles and internodes values:
    for t, scan in enumerate(scans_list):
        # - Read the `AnglesAndInternodes.json` file
        json_file = _get_files(scan, task_name, unique=True)[0]
        ae_dict = read_json(json_file)
        angles_and_internodes[scan.id] = {
            "angles": np.array(ae_dict["angles"]) * 180 / np.pi,
            "internodes": ae_dict["internodes"],
            "nb_organs": len(ae_dict["angles"])
        }
        nb_organs.append(len(ae_dict["angles"]))

    counter_nb_organs = Counter(nb_organs)
    print(" ** comparison results ** ")
    print("number of scans with the same nb of organs: ", counter_nb_organs)
    print("Min number of organs detected: ", min(nb_organs))
    print("Max number of organs detected: ", max(nb_organs))
    print("Average number of organs detected: ", np.mean(nb_organs))

    max_occurrence_nb_organs = counter_nb_organs.most_common()[0][0]
    angles = [angles_and_internodes[scan_num]["angles"] for scan_num in angles_and_internodes
              if angles_and_internodes[scan_num]["nb_organs"] == max_occurrence_nb_organs]
    internodes = [angles_and_internodes[scan_num]["internodes"] for scan_num in angles_and_internodes
                  if angles_and_internodes[scan_num]["nb_organs"] == max_occurrence_nb_organs]
    save_data_repartition(angles, "angles", pathlib.Path(db.basedir))
    save_data_repartition(internodes, "internodes", pathlib.Path(db.basedir))

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / f'{task_name}_comparison.json', 'w') as out_file:
        json.dump({
            'Min number of organs detected': min(nb_organs),
            'Max number of organs detected': max(nb_organs),
            'Average number of organs detected': np.mean(nb_organs),
            'number of scans with the same nb of organs': counter_nb_organs
        }, out_file)


def file_by_file_comparison(db, task_name):
    """
    Compares task folder output file by file, print result

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test

    """
    logger.info("Performing file-by-file comparisons...")
    fcmp = {}
    scans_list = [scan for scan in db.get_scans()]
    for t, ref_scan in enumerate(scans_list):
        if t + 1 > len(scans_list):
            break
        for flo_scan in scans_list[t + 1:]:
            ref_dir = pathlib.Path(db.basedir, ref_scan.id, _get_task_fileset(ref_scan, task_name).id)
            flo_dir = pathlib.Path(db.basedir, flo_scan.id, _get_task_fileset(flo_scan, task_name).id)
            n_diff = len(filecmp.dircmp(ref_dir, flo_dir).diff_files)
            n_same = len(filecmp.dircmp(ref_dir, flo_dir).same_files)
            if n_same == 0:
                similarity = 0
            else:
                similarity = (1 - n_diff / n_same) * 100
            fcmp[f"{ref_scan.id} - {flo_scan.id}"] = similarity

    # Print result to terminal:
    print("File-by-file comparison (similarity percentage) between pairs of repetitions:")
    print(fcmp)
    print()

    # Write a JSON summary file of the comparison:
    with open(pathlib.Path(db.basedir) / 'filebyfile_comparison.json', 'w') as out_file:
        json.dump({'Similarity (%)': fcmp}, out_file)


def compare_task_output(db, task_name, independant_tests=False):
    """
    Method to compare outputs of a task on replicated datasets.

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_name : string
        name of the task to test
    independant_tests : bool
        if ``True`` indicate all tasks were performed independently and will be evaluated, else only evaluate `task_name`

    Notes
    -----
    The behaviour of this method is configured by the JSON file `CONF_FILE`.

    """
    # List paths where tested task output(s) should be located
    folder_task_list = [pathlib.Path(db.basedir, scan.id, _get_task_fileset(scan, task_name).id) for scan in db.get_scans()]
    if len(folder_task_list) == 0:
        raise IOError(f"Output files of task {task_name} for db {db.basedir} are missing")
    # - Performs file-by-file comparisons for the selected task:
    file_by_file_comparison(db, task_name)
    # - Load the metric for the selected task from the JSON configuration file:
    c = json.load(open(CONF_FILE))
    # - Evaluate all tasks if independent tests, else evaluate only tested task:
    if independant_tests:
        for task in c.keys():
            try:
                eval(c[task]["comp_func"] + "(db, task)")
                print()  # make blocks of evaluation task summary clearer!
            except:
                pass
    else:
        eval(c[task_name]["comp_func"] + "(db, task_name)")


def fill_test_db(test_db, init_scan_path, task_cfg, nb):
    """
    From an initial scan, copy it in temporary folder, cleans it,
    runs the pipe to the comparison point task and copy the scan a certain
    number of times in a test folder

    Parameters
    ----------
    test_db : pathlib.Path
        name of the folder into which copy the tests scans
    init_scan_path : pathlib.Path
        path of the initial scan dataset to use for repeatability test
    task_cfg : dict
        name of the configuration file to run the pipeline
    nb : int
        number time to repeat the same task to evaluate

    Returns
    -------
    list
        list of pathlib.Path of created scan datasets

    """
    scan_name = init_scan_path.name
    created_copied_scans = []
    # Initialize a temporary directory use to store scan dataset before cleaning and running previous tasks to task to analyse:
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Get the path of the temporary ROMI database
        tmp_path = pathlib.Path(tmp_dir)
        tmp_scan_folder = pathlib.Path(tmp_path / f"tmp_{scan_name}")
        logger.info(f"Created a temporary folder '{tmp_scan_folder}'")
        # Creates the `romidb` marker for active FSDB
        with open(pathlib.Path(tmp_path / MARKER_FILE_NAME), 'w'):
            pass
        logger.info("Duplicating the scan dataset to a temporary folder...")
        # Duplicate the initial scan dataset to the temporary folder
        shutil.copytree(str(init_scan_path), str(tmp_scan_folder))
        # Check for previous analysis & if yes, clean the dataset
        init_configuration_file = pathlib.Path(tmp_scan_folder / "pipeline.toml")
        if init_configuration_file.is_file():
            logger.info("Using detected configuration pipeline backup file to clean the scan dataset!")
            print(str(init_configuration_file))
            run_pipe(tmp_scan_folder, "Clean", str(init_configuration_file))
        # Run the previous task on the temporary copy
        if task_cfg["previous_task"]:
            logger.info(f"Running {task_cfg['previous_task']} pipeline on the scan dataset!")
            run_pipe(tmp_scan_folder, task_cfg["previous_task"], task_cfg["config_file"])
        logger.info(f"Copying previous run of {task_cfg['previous_task']} pipeline...")
        # Duplicate the results of the previous task to the test database
        for i in range(nb):
            logger.info(f"Copy #{i}...")
            copied_scan_name = pathlib.Path(test_db / f"{scan_name}_{i}")
            shutil.copytree(str(tmp_scan_folder), str(copied_scan_name))
            created_copied_scans.append(copied_scan_name)
    return created_copied_scans


def create_test_db_path(root_location, task_name):
    """
    Generate test database name, ex: 20200803124841_rep_test_TriangleMesh

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the test database.
    task_name : str
        Name of the task to test.

    Returns
    -------
    pathlib.Path
        Path to the test database.
    """
    now = datetime.datetime.now()
    now_str = now.strftime("%Y%m%d%H%M%S")
    return pathlib.Path(root_location / f"{now_str}_rep_test_{task_name}")


def create_test_db(root_location, task_name):
    """
    Creates test db with `romidb` marker file in it.

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the test database.
    task_name : str
        Name of the task to test.

    Returns
    -------
    pathlib.Path
        Path to the test folder.

    """
    # Get the path of the ROMI database used to perform the test and creates the directory
    test_db = create_test_db_path(root_location, task_name)
    test_db.mkdir(exist_ok=True)
    # Creates the marker file to get an active romidata.FSDB:
    marker_file = test_db / MARKER_FILE_NAME
    marker_file.touch()
    return test_db


def run_pipe(scan_path, task_name, cfg_file):
    """
    run configured pipeline for given task on a scan

    Parameters
    ----------
    scan_path : pathlib.Path
        contains information on database
    task_name : string
        task name, must be a key in the CONF_FILE
    cfg_file : string
        name of the configuration file to run the pipeline

    Returns
    -------

    """
    logger.info(f"Executing tasks '{task_name}' on scan dataset '{str(scan_path)}'.")
    # TODO: use luigi.build() instead of subprocess.run call ?
    cmd = ["romi_run_task", "--config", cfg_file, task_name, str(scan_path), "--local-scheduler"]
    subprocess.run(cmd, check=True)
    return


def compute_repeatability_test(db, task_cfg):
    """
    Creates test folder with copies of the input scan and runs repeatability tests on it

    Parameters
    ----------
    db : romidata.FSDB
        Local ROMI database instance with the replicated scan datasets
    task_cfg : dict
        configuration dict of the task to test

    """
    scans_list = [scan.id for scan in db.get_scans()]
    logger.info(f"Repeating task {task_cfg['name']} on replicated scans list: {scans_list}")
    test_db.disconnect()  # to allow the luigi pipeline to run!
    # Execute the task to test for all scans in the test database:
    for copied_scan in scans_list:
        copied_scan_path = pathlib.Path(db.basedir, copied_scan)
        run_pipe(copied_scan_path, task_cfg["name"], task_cfg["config_file"])
    # Reconnect to the DB before leaving...
    test_db.connect()
    return


def config_task(task_name, cfg_file, full_pipe, previous_task):
    """
    Setup task configuration dictionary.

    Parameters
    ----------
    task_name : string
        task name, must be a key in the CONF_FILE
    cfg_file : string
        name of the configuration file to run the pipeline
    full_pipe : bool
        whether or not compute repeatability from the start
    previous_task : string
        name of the previous class in the pipeline, either None for the full pipe or the one linked to the class name
        in CONF_FILE (comparison point task)

    Returns
    -------
    dict
        configuration dict of the task to test

    """
    task_cfg = {
        "name": task_name,
        "config_file": cfg_file
    }
    if full_pipe:
        task_cfg["previous_task"] = None
    else:
        task_cfg["previous_task"] = previous_task
    return task_cfg


if __name__ == "__main__":
    """
    creates a test db at the root of the db linked to the scan to analyze
    """
    DESC = """ROMI reconstruction & analysis pipeline repeatability test procedure.

    Analyse the repeatability of a reconstruction & analysis pipeline by:
    1. duplicating the scan in a temporary folder (and cleaning it if necessary)
    2. running the pipeline up to the previous task of the task to test
    3. copying this result to a new database and replicate the dataset
    4. repeating the task to test for each replicate
    5. comparing the results pair by pair.

    Comparison can be done at the scale of the files but also with metrics if a reference can be set.
    
    To create fully independent tests, we run the pipeline up to the task to test on each replicate.
    """
    # - Load the JSON config file of the script:
    c = json.load(open(CONF_FILE))
    valid_tasks = list(c.keys())

    # TODO: an option to set a reference (or would that be the job of the Evaluation tasks?!)
    # TODO: use the `pipeline.toml` to defines the previous tasks ?

    parser = argparse.ArgumentParser(description=DESC)

    parser.add_argument("scan",
                        help="scan to use for repeatability analysis")
    parser.add_argument("task", default="AnglesAndInternodes",
                        help=f"task to test, should be in: {', '.join(valid_tasks)}")
    parser.add_argument("config_file",
                        help="path to the TOML config file of the analysis pipeline")
    parser.add_argument("-n", "--replicate_number", default=2,
                        help="number of replicate to use for repeatability analysis")
    parser.add_argument("-f", "--full_pipe", action="store_true", default=False,
                        help="run the analysis pipeline on each replicate independently")
    parser.add_argument("-np", "--no_pipeline", action="store_true", default=False,
                        help="do not run the pipeline, only compare tasks outputs")
    parser.add_argument("-db", '--test_database',
                        help="test database location to use. Use at your own risks!")
    # ->Test database path exists:
    #   - NO: create the directory & use it instead of auto-generated path
    #   - YES:
    #     ->Test database contain scan datasets
    #       - NO: use it instead of auto-generated path
    #       - YES: use number of scans as replicate number & compute repeatability test

    # - Parse the input arguments to variables:
    args = parser.parse_args()
    task2test = args.task
    replicate_number = int(args.replicate_number)
    init_scan_path = pathlib.Path(args.scan).expanduser()
    logger.info(f"Got scan path: {init_scan_path}")

    # - Test the task to test in valid:
    if task2test not in valid_tasks:
        raise ValueError(f"UNKNOWN task: '{task2test}', choose among: {', '.join(valid_tasks)}")

    # - Task configuration
    task_cfg = config_task(task2test, args.config_file, args.full_pipe, c[task2test]["prev_task"])

    # - 'test database' setup & dataset duplication:
    if args.test_database is not None:
        logger.info(f"Got a test database location as argument: '{args.test_database}'.")
        test_db_path = pathlib.Path(args.test_database)
        test_db_path.mkdir(exist_ok=True)
    else:
        # test db is created next to db containing original scan dataset
        root_location = init_scan_path.parent.parent
        test_db_path = create_test_db(root_location, task_cfg["name"])

    scan_replicates = list([idir.stem for idir in test_db_path.iterdir() if idir.is_dir()])
    n_replicates = len(scan_replicates)
    # Case without/with scan datasets in the test database root folder
    if n_replicates == 0:
        # Duplicate the scan dataset as many times as requested
        test_scans = fill_test_db(test_db_path, init_scan_path, task_cfg, replicate_number)
    else:
        # If there is something in the root folder that mean you passed a value to `--test_database`
        # And we then consider that it already contains the replicated scans!
        logger.info(f"Found {n_replicates} scan replicates in existing test database: {', '.join(map(str, scan_replicates))}")
        # Make sure the lock is OFF!
        lock_file = test_db_path / LOCK_FILE_NAME
        lock_file.unlink(missing_ok=True)

    # - Instantiate ROMI database
    test_db = FSDB(str(test_db_path))
    test_db.connect()
    if not args.no_pipeline:
        # - Repeat task on all replicated scans dataset of the test database:
        compute_repeatability_test(test_db, task_cfg)

    # - Compare the output(s) of the task across replicated scans dataset:
    compare_task_output(test_db, task_cfg["name"], independant_tests=args.full_pipe)
