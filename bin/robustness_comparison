#!/usr/bin/env python3
import argparse
import datetime
import filecmp
import json
import os
import pathlib
import shutil
import subprocess
import tempfile
from collections import Counter

import imageio
import matplotlib.pyplot as plt
import numpy as np
import logging
import open3d as o3d
from romiscan.metrics import chamfer_distance
from romiscan.metrics import point_cloud_registration_fitness
from romiscan.metrics import set_metrics
from romiscan.metrics import surface_ratio
from romiscan.metrics import volume_ratio

dirname, filename = os.path.split(os.path.abspath(__file__))
logger = logging.getLogger(f'{filename}')

CONF_FILE = os.path.join(dirname, "conf_robustness_comparison.json")


def _tests_pairs(pairs_dict: dict):
    tp = {}
    for pairs, test_result in pairs_dict.items():
        tp[({pairs[0].parts[-2]}, {pairs[1].parts[-2]})] = test_result
    return tp


def save_data_repartition(data, data_type, db):
    """
    Save repartition plots

    Parameters
    ----------
    data : list
        list of data to plot
    data_type : string
        angles or internodes
    db : pathlib.Path
        folder in which register the graphs

    """
    fig, ax = plt.subplots()
    ax.set_title(f'{data_type} distribution for same scan same pipe')
    ax.boxplot(list(np.array(data).transpose()))
    fig.savefig(db / f"repeat_results_{data_type}.png")


def compare_binary_mask(task_folder_output_list):
    precision, recall = {}, {}
    for t, ref_task in enumerate(task_folder_output_list):
        ref_masks = os.listdir(ref_task)
        for mask_id in ref_masks:
            mask = imageio.imread(ref_task / mask_id)
            if t + 1 > len(task_folder_output_list):
                break
            for flo_task in task_folder_output_list[t + 1:]:
                flo_mask = imageio.imread(ref_task / mask_id)
                _, _, _, _, precision[(ref_task, flo_task)], recall[(ref_task, flo_task)] = set_metrics(mask, flo_mask)

    print("\nPrecision (closer to 1.0 is better) between pairs of repetitions:")
    precision = _tests_pairs(precision)
    print(precision)
    json.dump({"precision": precision}, out_log)
    print("\nRecall (closer to 1.0 is better) between pairs of repetitions:")
    _tests_pairs(recall)
    return



def compare_pointcloud(task_folder_output_list):
    """Use the chamfer distance to compare point-cloud for each unique pairs of repetition."""
    chamfer_dist = {}
    fitness = {}
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_task in enumerate(task_folder_output_list):
        # - Read the `PointCloud.ply` file
        ref_pcd_file = ref_task / "PointCloud.ply"
        ref_pcd = o3d.io.read_point_cloud(str(ref_pcd_file))
        if t + 1 > len(task_folder_output_list):
            break
        for flo_task in task_folder_output_list[t + 1:]:
            # - Read the `PointCloud.ply` file
            flo_pcd_file = flo_task / "PointCloud.ply"
            flo_pcd = o3d.io.read_point_cloud(str(flo_pcd_file))
            # - Compute the Chamfer distance
            chamfer_dist[(ref_task, flo_task)] = chamfer_distance(ref_pcd, flo_pcd)
            fitness[(ref_task, flo_task)] = point_cloud_registration_fitness(ref_pcd, flo_pcd)

    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    _tests_pairs(chamfer_dist)
    print("\nFitness (closer to 1.0 is better) & residuals (inliner RMSE) between pairs of repetitions:")
    _tests_pairs(fitness)
    return


def compare_trianglemesh_points(task_folder_output_list):
    """Use the chamfer distance to compare mesh vertice point-cloud for each unique pairs of repetition."""
    chamfer_dist = {}
    surf_ratio = {}
    vol_ratio = {}
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_task in enumerate(task_folder_output_list):
        # - Read the `TriangleMesh.ply` file
        ref_mesh_file = ref_task / "TriangleMesh.ply"
        ref_mesh = o3d.io.read_triangle_mesh(str(ref_mesh_file))
        # - Extract a PointCloud from the mesh vertices
        ref_pcd = o3d.geometry.PointCloud(ref_mesh.vertices)
        if t + 1 > len(task_folder_output_list):
            break
        for flo_task in task_folder_output_list[t + 1:]:
            # - Read the `TriangleMesh.ply` file
            flo_mesh_file = flo_task / "TriangleMesh.ply"
            flo_mesh = o3d.io.read_triangle_mesh(str(flo_mesh_file))
            # - Extract a PointCloud from the mesh vertices
            flo_pcd = o3d.geometry.PointCloud(flo_mesh.vertices)
            # - Compute the Chamfer distance
            chamfer_dist[(ref_task, flo_task)] = chamfer_distance(ref_pcd, flo_pcd)
            surf_ratio[(ref_task, flo_task)] = surface_ratio(ref_mesh, flo_mesh)
            vol_ratio[(ref_task, flo_task)] = volume_ratio(ref_mesh, flo_mesh)

    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    _tests_pairs(chamfer_dist)
    print("\nSurface ratio (closer to 1.0 is better) between pairs of repetitions:")
    _tests_pairs(surf_ratio)
    print("\nVolume ratio (closer to 1.0 is better) between pairs of repetitions:")
    _tests_pairs(vol_ratio)
    return


def compare_curveskeleton_points(task_folder_output_list):
    """Use the chamfer distance to compare point-cloud for each unique pairs of repetition."""
    chamfer_dist = {}
    # - Double loop to compare all unique pairs of repeats:
    for t, ref_task in enumerate(task_folder_output_list):
        # - Read the `CurveSkeleton.json` file
        ref_mesh_file = ref_task / "CurveSkeleton.json"
        with open(str(ref_mesh_file), 'r') as jf:
            ref_mesh = json.load(jf)
        # - Extract a PointCloud from the skeleton vertices
        ref_pcd = o3d.geometry.PointCloud(ref_mesh["points"])
        if t + 1 > len(task_folder_output_list):
            break
        for flo_task in task_folder_output_list[t + 1:]:
            # - Read the `CurveSkeleton.json` file
            flo_mesh_file = flo_task / "CurveSkeleton.json"
            with open(str(flo_mesh_file), 'r') as jf:
                flo_mesh = json.load(jf)
            # - Extract a PointCloud from the skeleton vertices
            flo_pcd = o3d.geometry.PointCloud(flo_mesh["points"])
            # - Compute the Chamfer distance
            chamfer_dist[(ref_task, flo_task)] = chamfer_distance(ref_pcd, flo_pcd)

    print("\nChamfer distance (lower is better) between pairs of repetitions:")
    _tests_pairs(chamfer_dist)
    return


def angles_and_internodes_comparison(task_folder_output_list, test_db):
    """
    Coarse comparison of sequences of angles and internodes
    Prints a dict containing the number of scans for each number of organs indexed, ex : {12:4, 14:6} (4 scans with 12
    organs recognized and 6 with 14 organs)
    For the biggest number of scans with same number of organs, creates plots of the repartition of angles (and
    internodes) for each organ id

    Parameters
    ----------
    task_folder_output_list : pathlib.Path list
        list of task output path to compare
    test_db : pathlib.Path
        path to put the potential results files

    """
    angles_and_internodes = {}
    nb_organs = []
    ae_file_list = [f / "AnglesAndInternodes.json" for f in task_folder_output_list]
    for ae_file in ae_file_list:
        with open(ae_file) as f:
            ae_list = json.load(f)
            angles_and_internodes[str(ae_file.parent.parent)[-1]] = {
                "angles": np.array(ae_list["angles"]) * 180 / np.pi,
                "internodes": ae_list["internodes"],
                "nb_organs": len(ae_list["angles"])
            }
            nb_organs.append(len(ae_list["angles"]))
    counter_nb_organs = Counter(nb_organs)
    print(" ** comparison results ** ")
    print("number of scans with the same nb of organs: ", counter_nb_organs)
    max_occurrence_nb_organs = counter_nb_organs.most_common()[0][0]
    angles = [angles_and_internodes[scan_num]["angles"] for scan_num in angles_and_internodes
              if angles_and_internodes[scan_num]["nb_organs"] == max_occurrence_nb_organs]
    internodes = [angles_and_internodes[scan_num]["internodes"] for scan_num in angles_and_internodes
                  if angles_and_internodes[scan_num]["nb_organs"] == max_occurrence_nb_organs]
    save_data_repartition(angles, "angles", test_db)
    save_data_repartition(internodes, "internodes", test_db)


def file_by_file_comparison(task_folder_output_list):
    """
    Compares task folder output file by file, print result

    Parameters
    ----------
    task_folder_output_list : pathlib.Path list
        list of task output path to compare

    """
    other_folders = task_folder_output_list[:]
    identical_folders_list = []
    for current_task_output in task_folder_output_list:
        if current_task_output in other_folders:
            current_identical_folder_list = [f for f in other_folders
                                             if not len(filecmp.dircmp(str(current_task_output), str(f)).diff_files)]
            identical_folders_list.append(current_identical_folder_list)
            other_folders = list(set(other_folders) - set(current_identical_folder_list))
        else:
            pass
    print(" ** comparison results ** ")
    print(f"compare_file {identical_folders_list}")
    print(np.array(identical_folders_list).shape)


def get_task_folder(scan, task_name):
    """
    Look for the output folder of a task in the files.json file of a scan returns name as a string of a task output folder

    Parameters
    ----------
    scan
    task_name

    Returns
    -------

    """
    files = json.load(open(str(scan / "files.json")))
    task_folders = [d["id"] for d in files["filesets"] if task_name in d["id"]]
    return task_folders[0]  # shame ...


def compare_task_output(scan_list, task_name, previous_task, test_db):
    """
    Method to compare outputs of a task.

    Parameters
    ----------
    scan_list : list(pathlib.Path)
        List of paths to the test scans.
    task_name : string
        name of the task to test
    previous_task : string
        name of the task which is the comparison point
    test_db : pathlib.Path
        Path to the test folder.

    Notes
    -----
    The behaviour of this method is configured by the JSON file `CONF_FILE`.

    """
    task_folder = get_task_folder(scan_list[0], task_name)
    folder_task_list = [scan_path / task_folder for scan_path in scan_list]
    if previous_task is None and task_name == "AnglesAndInternodes" and len(folder_task_list):
        angles_and_internodes_comparison(folder_task_list, test_db)
    elif len(folder_task_list):
        file_by_file_comparison(folder_task_list)
    else:
        print(f"Output files of task {task_name} for db {str(scan_list[0].parent)} are missing")
    # - Load the metric from the JSON configuration file
    c = json.load(open(CONF_FILE))
    eval(c[task_name]["comp_func"] + "(folder_task_list)")


def fill_test_db(test_db, init_scan_path, task_cfg, nb):
    """
    From an initial scan, copy it in temporary folder, cleans it,
    runs the pipe to the comparison point task and copy the scan a certain
    number of times in a test folder

    Parameters
    ----------
    test_db : pathlib.Path
        name of the folder into which copy the tests scans
    init_scan_path : pathlib.Path
        path of the initial scan
    task_cfg : dict
        name of the configuration file to run the pipeline
    nb : int
        number time to repeat the same task to evaluate

    Returns
    -------
    list
        list of pathlib.Path of created scans

    """
    scan_name = init_scan_path.name
    created_copied_scans = []
    # Initialize a temporary directory use to store scan dataset before cleaning and running previous tasks to task to analyse:
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Get the path of the temporary ROMI database
        tmp_path = pathlib.Path(tmp_dir)
        tmp_scan_folder = pathlib.Path(tmp_path / f"tmp_{scan_name}")
        logger.info(f"Created a temporary folder '{tmp_scan_folder}'")
        # Creates the `romidb` marker for active FSDB
        with open(pathlib.Path(tmp_path / "romidb"), 'w'):
            pass
        logger.info("Duplicating the scan dataset to a temporary folder...")
        # Duplicate the initial scan dataset to the temporary folder
        shutil.copytree(str(init_scan_path), str(tmp_scan_folder))
        # Check for previous analysis & if yes, clean the dataset
        init_configuration_file = pathlib.Path(tmp_scan_folder / "pipeline.toml")
        if init_configuration_file.is_file():
            logger.info("Using detected configuration pipeline backup file to clean the scan dataset!")
            print(str(init_configuration_file))
            run_pipe(tmp_scan_folder, "Clean", str(init_configuration_file))
        # Run the previous task on the temporary copy
        if task_cfg["previous_task"]:
            logger.info(f"Running {task_cfg['previous_task']} pipeline on the scan dataset!")
            run_pipe(tmp_scan_folder, task_cfg["previous_task"], task_cfg["config_file"])
        logger.info(f"Copying previous run of {task_cfg['previous_task']} pipeline...")
        # Duplicate the results of the previous task to the test database
        for i in range(nb):
            logger.info(f"Copy #{i}...")
            copied_scan_name = pathlib.Path(test_db / f"{scan_name}_{i}")
            shutil.copytree(str(tmp_scan_folder), str(copied_scan_name))
            created_copied_scans.append(copied_scan_name)
    return created_copied_scans


def get_test_db_path(root_location, task_name):
    """
    Generate test database name, ex: 20200803124841_rep_test_TriangleMesh

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the test database.
    task_name : str
        Name of the task to test.

    Returns
    -------
    pathlib.Path
        Path to the test database.
    """
    now = datetime.datetime.now()
    now_str = now.strftime("%Y%m%d%H%M%S")
    return pathlib.Path(root_location / f"{now_str}_rep_test_{task_name}")


def create_test_db(root_location, task_name):
    """
    Creates test db with `romidb` marker file in it.

    Parameters
    ----------
    root_location : pathlib.Path
        Root path where to create the test database.
    task_name : str
        Name of the task to test.

    Returns
    -------
    pathlib.Path
        Path to the test folder.

    """
    # Get the path of the ROMI database used to perform the test and creates the directory
    test_db = get_test_db_path(root_location, task_name)
    test_db.mkdir(exist_ok=True)
    # Creates the `romidb` marker for active FSDB
    with open(pathlib.Path(test_db / "romidb"), mode='w'):
        pass
    return test_db


def run_pipe(scan_path, task_name, cfg_file):
    """
    run confiured pipeline for given task on a scan

    Parameters
    ----------
    scan_path : pathlib.Path
        contains information on database
    task_name : string
        task name, must be a key in the CONF_FILE
    cfg_file : string
        name of the configuration file to run the pipeline

    Returns
    -------

    """
    logger.info(f"Executing tasks '{task_name}' on scan dataset '{str(scan_path)}'.")
    # TODO: use luigi.build() instead of subprocess.run call
    cmd = ["romi_run_task", "--config", cfg_file, task_name, str(scan_path), "--local-scheduler"]
    subprocess.run(cmd, check=True)


def compute_repeatability_test(task_cfg):
    """
    Creates test folder with copies of the input scan and runs repeatability tests on it

    Parameters
    ----------
    task_cfg : dict
        configuration dict of the task to test

    """
    for copied_scan in test_scans_list:
        run_pipe(copied_scan, task_cfg["name"], task_cfg["config_file"])
    compare_task_output(test_scans_list, task_cfg["name"], task_cfg["previous_task"], test_db)


def config_task(task_name, cfg_file, full_pipe, previous_task):
    """
    Setup task configuration dictionary.

    Parameters
    ----------
    task_name : string
        task name, must be a key in the CONF_FILE
    cfg_file : string
        name of the configuration file to run the pipeline
    full_pipe : bool
        whether or not compute repeatability from the start
    previous_task : string
        name of the previous class in the pipeline, either None for the full pipe or the one linked to the class name
        in CONF_FILE (comparison point task)

    Returns
    -------
    dict
        configuration dict of the task to test

    """
    task_cfg = {
        "name": task_name,
        "config_file": cfg_file
    }
    if full_pipe:
        task_cfg["previous_task"] = None
    else:
        task_cfg["previous_task"] = previous_task
    return task_cfg


if __name__ == "__main__":
    """
    creates a test db at the root of the db linked to the scan to analyze
    """
    DESC = """ROMI reconstruction & analysis pipeline repeatability test procedure.

    Analyse the repeatability of a reconstruction & analysis pipeline by:
    1. duplicating the scan in a temporary folder (and cleaning it if necessary)
    2. running the pipeline up to the previous task of the task to test
    3. copying this result to a new database and replicate the dataset
    4. repeating the task to test for each replicate
    5. comparing the results pair by pair.

    Comparison can be done at the scale of the files but also with metrics if a reference can be set.
    
    To create fully independent tests, *i.e.* we the whole pipeline in run up to the task to test on each replicate.
    
    """
    # - Load the JSON config file of the script:
    c = json.load(open(CONF_FILE))
    valid_tasks = list(c.keys())

    # TODO: an option to set a reference (or would that be the job of the Evaluation tasks?!)
    # TODO: add 'output_folder' argument to re-use previous test DB.

    parser = argparse.ArgumentParser(description=DESC)

    parser.add_argument("scan",
                        help="scan to use for repeatability analysis")
    parser.add_argument("task", default="AnglesAndInternodes",
                        help=f"task to test, should be in: {', '.join(valid_tasks)}")
    parser.add_argument("config_file",
                        help="path to the TOML config file of the analysis pipeline")
    parser.add_argument("-n", "--replicate_number", default=2,
                        help="number of replicate to use for repeatability analysis")
    parser.add_argument("-f", "--full_pipe", action="store_true", default=False,
                        help="run the analysis pipeline on each replicate independently")

    # - Parse the input agruments to variables:
    args = parser.parse_args()
    task2test = args.task
    replicate_number = int(args.replicate_number)
    init_scan_path = pathlib.Path(args.scan).expanduser()
    logger.info(f"Got scan path: {init_scan_path}")

    # - Test the task to test in valid:
    if task2test not in valid_tasks:
        raise ValueError(f"UNKNOWN task: '{task2test}', choose among: {', '.join(valid_tasks)}")

    task_cfg = config_task(task2test, args.config_file, args.full_pipe, c[task2test]["prev_task"])

    root_location = init_scan_path.parent.parent  # FSDB root location
    test_db = create_test_db(root_location, task_cfg["name"])
    # Duplicate the scan dataset as many times as requested
    test_scans = fill_test_db(test_db, init_scan_path, task_cfg, replicate_number)

    compute_repeatability_test(test_db, task_cfg, replicate_number)

    # TODO: create a log file with the comparisons scores.